{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "16680599\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "<class 'list'>\n",
      "16680599\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n",
      "anarchism\n",
      "5234\n",
      "39\n",
      "63642\n",
      "mOne 8209818\n",
      "mTwo 12346548\n",
      "mTre 8146176\n",
      "mFoo 8146176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:m266vfn5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▂▇▄▅▂▄▇▇▃▄▆▅▇▄█▇▂▅▄▇▄▄▅█▆▅▅▆▄▅▇█▆▄▆▇▄▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.60369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-wood-19</strong> at: <a href='https://wandb.ai/omareweis123/cbow_training/runs/m266vfn5' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/m266vfn5</a><br/> View project at: <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241016_200822-m266vfn5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:m266vfn5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\hackernews_predicting_views\\data processing\\wandb\\run-20241016_200926-iabpu5nz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/cbow_training/runs/iabpu5nz' target=\"_blank\">silvery-capybara-20</a></strong> to <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/cbow_training/runs/iabpu5nz' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/iabpu5nz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▃▂▃▂▄▂▄▂▂▂▃▂▁▁▂▂▂█                     </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-capybara-20</strong> at: <a href='https://wandb.ai/omareweis123/cbow_training/runs/iabpu5nz' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/iabpu5nz</a><br/> View project at: <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241016_200926-iabpu5nz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "with open('text8') as f: text8: str = f.read()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  text = text.replace('.',  ' <PERIOD> ')\n",
    "  text = text.replace(',',  ' <COMMA> ')\n",
    "  text = text.replace('\"',  ' <QUOTATION_MARK> ')\n",
    "  text = text.replace(';',  ' <SEMICOLON> ')\n",
    "  text = text.replace('!',  ' <EXCLAMATION_MARK> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace('(',  ' <LEFT_PAREN> ')\n",
    "  text = text.replace(')',  ' <RIGHT_PAREN> ')\n",
    "  text = text.replace('--', ' <HYPHENS> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace(':',  ' <COLON> ')\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 5]\n",
    "  return words\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "corpus: list[str] = preprocess(text8)\n",
    "print(type(corpus)) # <class 'list'>\n",
    "print(len(corpus))  # 16,680,599\n",
    "print(corpus[:7])   # ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "  word_counts = collections.Counter(words)\n",
    "  vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "  int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "  int_to_vocab[0] = '<PAD>'\n",
    "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "  return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "words_to_ids, ids_to_words = create_lookup_tables(corpus)\n",
    "tokens = [words_to_ids[word] for word in corpus]\n",
    "print(type(tokens)) # <class 'list'>\n",
    "print(len(tokens))  # 16,680,599\n",
    "print(tokens[:7])   # [5234, 3081, 12, 6, 195, 2, 3134]\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print(ids_to_words[5234])        # anarchism\n",
    "print(words_to_ids['anarchism']) # 5234\n",
    "print(words_to_ids['have'])      # 3081\n",
    "print(len(words_to_ids))         # 63,642\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramOne(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, _):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    out = self.ffw(emb)\n",
    "    sft = self.max(out)\n",
    "    return -(sft[0, trgs]).log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTwo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=ctx*voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    hid = self.ffw(emb)\n",
    "    lgt = hid.view(self.ctx, -1)\n",
    "    sft = self.max(lgt)\n",
    "    arg = torch.arange(sft.size(0))\n",
    "    foo = sft[arg, trgs]\n",
    "    return -foo.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTre(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    ctx = self.ffw.weight[trgs]\n",
    "    lgt = torch.mm(ctx, emb.T)\n",
    "    sig = self.sig(lgt)\n",
    "    return -sig.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inpt, trgs, rand):\n",
    "    emb = self.emb(inpt)\n",
    "    ctx = self.ffw.weight[trgs]\n",
    "    rnd = self.ffw.weight[rand]\n",
    "    out = torch.mm(ctx, emb.T)\n",
    "    rnd = torch.mm(rnd, emb.T)\n",
    "    out = self.sig(out)\n",
    "    rnd = self.sig(rnd)\n",
    "    pst = -out.log().mean()\n",
    "    ngt = -(1 - rnd).log().mean()\n",
    "    return pst + ngt\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "args = (len(words_to_ids), 64, 2)\n",
    "mOne = SkipGramOne(*args)\n",
    "mTwo = SkipGramTwo(*args)\n",
    "mTre = SkipGramTre(*args)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print('mOne', sum(p.numel() for p in mOne.parameters()))\n",
    "print('mTwo', sum(p.numel() for p in mTwo.parameters()))\n",
    "print('mTre', sum(p.numel() for p in mTre.parameters()))\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "opOne = torch.optim.Adam(mOne.parameters(), lr=0.003)\n",
    "opTwo = torch.optim.Adam(mTwo.parameters(), lr=0.003)\n",
    "opTre = torch.optim.Adam(mTre.parameters(), lr=0.003)\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mOne')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     opOne.zero_grad()\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     loss = mOne(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opOne.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTwo')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(wins, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTwo.zero_grad()\n",
    "#     loss = mTwo(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTwo.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTre')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTre.zero_grad()\n",
    "#     loss = mTre(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTre.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Weights and Biases\n",
    "wandb.init(project=\"cbow_training\", entity=\"omareweis123\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "mFoo = mFoo.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "    prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for i, tks in prgs:\n",
    "        # Move input tensors to the same device (GPU or CPU)\n",
    "        inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "        trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "        rand = torch.randint(0, len(words_to_ids), (2,)).to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = mFoo(inpt, trgs, rand)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item()})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# wandb.init(project=\"cbow_training\", entity=\"omareweis123\")\n",
    "\n",
    "# # Define your token limit here (e.g., 10,000 tokens)\n",
    "# token_limit = 10000\n",
    "\n",
    "# # Split data into 80% train, 20% validation from the limited set\n",
    "# train_tokens = tokens[:int(0.8 * token_limit)]\n",
    "# val_tokens = tokens[int(0.8 * token_limit):token_limit]\n",
    "\n",
    "# # Move the model to the correct device\n",
    "# mFoo = mFoo.to(device)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     # Training loop\n",
    "#     mFoo.train()\n",
    "#     train_wins = list(more_itertools.windowed(train_tokens, 3))  # Create training windows\n",
    "#     train_prgs = tqdm.tqdm(enumerate(train_wins), total=len(train_wins), desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "#     train_loss_total = 0\n",
    "#     for i, tks in train_prgs:\n",
    "#         if None in tks:  # Skip invalid windows (e.g., at the end)\n",
    "#             continue\n",
    "#         inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#         trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#         rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#         opFoo.zero_grad()\n",
    "#         loss = mFoo(inpt, trgs, rand)\n",
    "#         loss.backward()\n",
    "#         opFoo.step()\n",
    "#         train_loss_total += loss.item()\n",
    "#         wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "#     avg_train_loss = train_loss_total / len(train_wins)\n",
    "#     wandb.log({'avg_train_loss': avg_train_loss})\n",
    "\n",
    "#     # Validation loop\n",
    "#     mFoo.eval()\n",
    "#     val_wins = list(more_itertools.windowed(val_tokens, 3))  # Create validation windows\n",
    "#     val_prgs = tqdm.tqdm(enumerate(val_wins), total=len(val_wins), desc=f\"Epoch {epoch+1} [Validation]\", leave=False)\n",
    "#     val_loss_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, tks in val_prgs:\n",
    "#             if None in tks:  # Skip invalid windows\n",
    "#                 continue\n",
    "#             inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#             trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#             rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#             val_loss = mFoo(inpt, trgs, rand)\n",
    "#             val_loss_total += val_loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss_total / len(val_wins)\n",
    "#     wandb.log({'avg_val_loss': avg_val_loss})\n",
    "\n",
    "# wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
