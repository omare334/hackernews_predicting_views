{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "16680599\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "<class 'list'>\n",
      "16680599\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n",
      "anarchism\n",
      "5234\n",
      "39\n",
      "63642\n",
      "mOne 8209818\n",
      "mTwo 12346548\n",
      "mTre 8146176\n",
      "mFoo 8146176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\hackernews_predicting_views\\data processing\\wandb\\run-20241017_122004-2172kx8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/cbow_training/runs/2172kx8v' target=\"_blank\">batch_size(32)</a></strong> to <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/cbow_training/runs/2172kx8v' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/2172kx8v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▂▁▃▄█▂▅▄▅▂▃▄▄▅▂▄▂▃▂▂▄▃▃▂▂▃▃▁▅▁▂▄▂▃▄▂▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.56962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size(32)</strong> at: <a href='https://wandb.ai/omareweis123/cbow_training/runs/2172kx8v' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/2172kx8v</a><br/> View project at: <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241017_122004-2172kx8v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "with open('text8') as f: text8: str = f.read()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  text = text.replace('.',  ' <PERIOD> ')\n",
    "  text = text.replace(',',  ' <COMMA> ')\n",
    "  text = text.replace('\"',  ' <QUOTATION_MARK> ')\n",
    "  text = text.replace(';',  ' <SEMICOLON> ')\n",
    "  text = text.replace('!',  ' <EXCLAMATION_MARK> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace('(',  ' <LEFT_PAREN> ')\n",
    "  text = text.replace(')',  ' <RIGHT_PAREN> ')\n",
    "  text = text.replace('--', ' <HYPHENS> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace(':',  ' <COLON> ')\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 5]\n",
    "  return words\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "corpus: list[str] = preprocess(text8)\n",
    "print(type(corpus)) # <class 'list'>\n",
    "print(len(corpus))  # 16,680,599\n",
    "print(corpus[:7])   # ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "  word_counts = collections.Counter(words)\n",
    "  vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "  int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "  int_to_vocab[0] = '<PAD>'\n",
    "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "  return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "words_to_ids, ids_to_words = create_lookup_tables(corpus)\n",
    "tokens = [words_to_ids[word] for word in corpus]\n",
    "print(type(tokens)) # <class 'list'>\n",
    "print(len(tokens))  # 16,680,599\n",
    "print(tokens[:7])   # [5234, 3081, 12, 6, 195, 2, 3134]\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print(ids_to_words[5234])        # anarchism\n",
    "print(words_to_ids['anarchism']) # 5234\n",
    "print(words_to_ids['have'])      # 3081\n",
    "print(len(words_to_ids))         # 63,642\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramOne(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, _):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    out = self.ffw(emb)\n",
    "    sft = self.max(out)\n",
    "    return -(sft[0, trgs]).log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTwo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=ctx*voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    hid = self.ffw(emb)\n",
    "    lgt = hid.view(self.ctx, -1)\n",
    "    sft = self.max(lgt)\n",
    "    arg = torch.arange(sft.size(0))\n",
    "    foo = sft[arg, trgs]\n",
    "    return -foo.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTre(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    ctx = self.ffw.weight[trgs]\n",
    "    lgt = torch.mm(ctx, emb.T)\n",
    "    sig = self.sig(lgt)\n",
    "    return -sig.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  # def forward(self, inpt, trgs, rand):\n",
    "  #   emb = self.emb(inpt)\n",
    "  #   ctx = self.ffw.weight[trgs]\n",
    "  #   rnd = self.ffw.weight[rand]\n",
    "  #   out = torch.mm(ctx, emb.T)\n",
    "  #   rnd = torch.mm(rnd, emb.T)\n",
    "  #   out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   pst = -out.log().mean()\n",
    "  #   ngt = -(1 - rnd).log().mean()\n",
    "  #   return pst + ngt\n",
    "\n",
    "#new forwarding for batch size \n",
    "  def forward(self, inpt, trgs, rand):\n",
    "    # Embedding lookup for input (shape: [batch_size, embedding_dim])\n",
    "    emb = self.emb(inpt)\n",
    "    \n",
    "    # Ensure context (trgs) and random samples (rand) have the same batch size as inpt\n",
    "    batch_size = inpt.size(0)  # Get the current batch size\n",
    "\n",
    "    # Slice or generate the random tensor according to the input batch size\n",
    "    rand = rand[:batch_size]  # Adjust random tensor to match current batch size\n",
    "    \n",
    "    ctx = self.ffw.weight[trgs.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "    rnd = self.ffw.weight[rand.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "\n",
    "    # Ensure the batch size matches before performing batch matrix multiplication\n",
    "    assert ctx.size(0) == emb.size(0), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    assert rnd.size(0) == emb.size(0), f\"Random batch size {rnd.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    out = torch.bmm(ctx, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    rnd = torch.bmm(rnd, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Apply sigmoid and clamp to prevent NaNs\n",
    "    out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "    rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "    # Calculate loss\n",
    "    pst = -out.log().mean()   # Positive sample log-likelihood\n",
    "    ngt = -(1 - rnd).log().mean()  # Negative sample log-likelihood\n",
    "    \n",
    "    return pst + ngt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "args = (len(words_to_ids), 64, 2)\n",
    "mOne = SkipGramOne(*args)\n",
    "mTwo = SkipGramTwo(*args)\n",
    "mTre = SkipGramTre(*args)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print('mOne', sum(p.numel() for p in mOne.parameters()))\n",
    "print('mTwo', sum(p.numel() for p in mTwo.parameters()))\n",
    "print('mTre', sum(p.numel() for p in mTre.parameters()))\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "opOne = torch.optim.Adam(mOne.parameters(), lr=0.003)\n",
    "opTwo = torch.optim.Adam(mTwo.parameters(), lr=0.003)\n",
    "opTre = torch.optim.Adam(mTre.parameters(), lr=0.003)\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mOne')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     opOne.zero_grad()\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     loss = mOne(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opOne.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTwo')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(wins, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTwo.zero_grad()\n",
    "#     loss = mTwo(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTwo.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTre')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTre.zero_grad()\n",
    "#     loss = mTre(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTre.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "wandb.init(project=\"cbow_training\", entity=\"omareweis123\",name='batch_size(32)')\n",
    "# Set batch size\n",
    "batch_size = 128\n",
    "mFoo = mFoo.to(device)\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    wins = list(more_itertools.windowed(tokens[:10000000], window_size))  # Convert to list for easier batching\n",
    "    prgs = tqdm.tqdm(range(0, len(wins), batch_size), total=len(wins)//batch_size, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    \n",
    "    for batch_idx in prgs:\n",
    "        batch_wins = wins[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Prepare batch inputs and targets, ensuring they're on the correct device\n",
    "        inpts = torch.LongTensor([win[context_size] for win in batch_wins]).to(device)  # Central token for each window\n",
    "        trgs = torch.LongTensor([[win[i] for i in range(context_size)] + [win[i] for i in range(context_size + 1, window_size)]\n",
    "        for win in batch_wins]).to(device) # Context tokens (left and right)\n",
    "        rand = torch.randint(0, len(words_to_ids), (batch_size, 2)).to(device)  # Random negative samples on the same device\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = mFoo(inpts, trgs, rand)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item()})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "## this is without batch size \n",
    "# # Initialize Weights and Biases\n",
    "# wandb.init(project=\"cbow_training\", entity=\"omareweis123\",name='clamped_logs')\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# mFoo = mFoo.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(10):\n",
    "#     wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#     prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#     for i, tks in prgs:\n",
    "#         # Move input tensors to the same device (GPU or CPU)\n",
    "#         inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#         trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#         rand = torch.randint(0, len(words_to_ids), (2,)).to(device)\n",
    "\n",
    "#         # Zero gradients\n",
    "#         opFoo.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         loss = mFoo(inpt, trgs, rand)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         opFoo.step()\n",
    "\n",
    "#         # Log the loss\n",
    "#         wandb.log({'loss': loss.item()})\n",
    "\n",
    "# # Finish the W&B logging\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# wandb.init(project=\"cbow_training\", entity=\"omareweis123\")\n",
    "\n",
    "# # Define your token limit here (e.g., 10,000 tokens)\n",
    "# token_limit = 10000\n",
    "\n",
    "# # Split data into 80% train, 20% validation from the limited set\n",
    "# train_tokens = tokens[:int(0.8 * token_limit)]\n",
    "# val_tokens = tokens[int(0.8 * token_limit):token_limit]\n",
    "\n",
    "# # Move the model to the correct device\n",
    "# mFoo = mFoo.to(device)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     # Training loop\n",
    "#     mFoo.train()\n",
    "#     train_wins = list(more_itertools.windowed(train_tokens, 3))  # Create training windows\n",
    "#     train_prgs = tqdm.tqdm(enumerate(train_wins), total=len(train_wins), desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "#     train_loss_total = 0\n",
    "#     for i, tks in train_prgs:\n",
    "#         if None in tks:  # Skip invalid windows (e.g., at the end)\n",
    "#             continue\n",
    "#         inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#         trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#         rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#         opFoo.zero_grad()\n",
    "#         loss = mFoo(inpt, trgs, rand)\n",
    "#         loss.backward()\n",
    "#         opFoo.step()\n",
    "#         train_loss_total += loss.item()\n",
    "#         wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "#     avg_train_loss = train_loss_total / len(train_wins)\n",
    "#     wandb.log({'avg_train_loss': avg_train_loss})\n",
    "\n",
    "#     # Validation loop\n",
    "#     mFoo.eval()\n",
    "#     val_wins = list(more_itertools.windowed(val_tokens, 3))  # Create validation windows\n",
    "#     val_prgs = tqdm.tqdm(enumerate(val_wins), total=len(val_wins), desc=f\"Epoch {epoch+1} [Validation]\", leave=False)\n",
    "#     val_loss_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, tks in val_prgs:\n",
    "#             if None in tks:  # Skip invalid windows\n",
    "#                 continue\n",
    "#             inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#             trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#             rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#             val_loss = mFoo(inpt, trgs, rand)\n",
    "#             val_loss_total += val_loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss_total / len(val_wins)\n",
    "#     wandb.log({'avg_val_loss': avg_val_loss})\n",
    "\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'house':\n",
      "house: 1.0000\n",
      "disciplinary: 0.5499\n",
      "courant: 0.5117\n",
      "cernunnos: 0.5052\n",
      "fauntleroy: 0.5012\n",
      "hatti: 0.5011\n",
      "avenge: 0.4971\n",
      "panicked: 0.4829\n",
      "casaubon: 0.4803\n",
      "acquisitions: 0.4774\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'house'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Initialize a list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Loop through all words in your vocabulary to calculate cosine similarity\n",
    "for idx in range(len(words_to_ids)):\n",
    "    # Get the embedding for each word\n",
    "    current_embedding = mFoo.emb(torch.LongTensor([idx]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(target_embedding, current_embedding)  # Shape: [1]\n",
    "    similarities.append((ids_to_words[idx], similarity.item()))  # Store the word and its similarity\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities[:10]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words similar to '{}':\".format(word_to_check))\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'house':\n",
      "house: 1.0000\n",
      "home: 0.5793\n",
      "gilligan: 0.5735\n",
      "george: 0.5618\n",
      "city: 0.5519\n",
      "delano: 0.5358\n",
      "born: 0.5344\n",
      "museum: 0.5340\n",
      "palatine: 0.5323\n",
      "york: 0.5299\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'house'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Initialize a list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Loop through all words in your vocabulary to calculate cosine similarity\n",
    "for idx in range(len(words_to_ids)):\n",
    "    # Get the embedding for each word\n",
    "    current_embedding = mFoo.emb(torch.LongTensor([idx]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(target_embedding, current_embedding)  # Shape: [1]\n",
    "    similarities.append((ids_to_words[idx], similarity.item()))  # Store the word and its similarity\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities[:10]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words similar to '{}':\".format(word_to_check))\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
