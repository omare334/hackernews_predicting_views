{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "19214318\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "<class 'list'>\n",
      "19214318\n",
      "[6078, 3612, 13, 7, 226, 2, 2811]\n",
      "eggs\n",
      "6078\n",
      "39\n",
      "70572\n",
      "mOne 9103788\n",
      "mTwo 13690968\n",
      "mTre 9033216\n",
      "mFoo 9033216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33momareweis123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\hackernews_predicting_views\\data processing\\wandb\\run-20241018_114716-zcorsddo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/cbow_training/runs/zcorsddo' target=\"_blank\">batch_size(4096),tokens1000000000,30epochs,titlesadded</a></strong> to <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/cbow_training/runs/zcorsddo' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/zcorsddo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▂▂▃▂▂▂▃▂▃▂▃▂▂▂▂▃▂▁▂▁▃▃▂▂▂▂▃▃▂▂▃▃▃▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>0.42727</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>0.49783</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size(4096),tokens1000000000,30epochs,titlesadded</strong> at: <a href='https://wandb.ai/omareweis123/cbow_training/runs/zcorsddo' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/zcorsddo</a><br/> View project at: <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_114716-zcorsddo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "with open('text8') as f: text8: str = f.read()\n",
    "\n",
    "# Load the CSV file and extract the 'title_cleaned' column\n",
    "df = pd.read_csv('titles_20_scores')\n",
    "titles_cleaned = df['title_cleaned'].tolist()\n",
    "\n",
    "# Convert the list of titles to a single string (you can choose a delimiter)\n",
    "titles_string = ' '.join(titles_cleaned)  # Joining with a space\n",
    "\n",
    "# Concatenate the titles string to the text8 variable\n",
    "text8 += ' ' + titles_string  # Add a space for separation\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  text = text.replace('.',  ' <PERIOD> ')\n",
    "  text = text.replace(',',  ' <COMMA> ')\n",
    "  text = text.replace('\"',  ' <QUOTATION_MARK> ')\n",
    "  text = text.replace(';',  ' <SEMICOLON> ')\n",
    "  text = text.replace('!',  ' <EXCLAMATION_MARK> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace('(',  ' <LEFT_PAREN> ')\n",
    "  text = text.replace(')',  ' <RIGHT_PAREN> ')\n",
    "  text = text.replace('--', ' <HYPHENS> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace(':',  ' <COLON> ')\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 5]\n",
    "  return words\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "corpus: list[str] = preprocess(text8)\n",
    "print(type(corpus)) # <class 'list'>\n",
    "print(len(corpus))  # 16,680,599\n",
    "print(corpus[:7])   # ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "  word_counts = collections.Counter(words)\n",
    "  vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "  int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "  int_to_vocab[0] = '<PAD>'\n",
    "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "  return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "words_to_ids, ids_to_words = create_lookup_tables(corpus)\n",
    "tokens = [words_to_ids[word] for word in corpus]\n",
    "print(type(tokens)) # <class 'list'>\n",
    "print(len(tokens))  # 16,680,599\n",
    "print(tokens[:7])   # [5234, 3081, 12, 6, 195, 2, 3134]\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print(ids_to_words[5234])        # anarchism\n",
    "print(words_to_ids['anarchism']) # 5234\n",
    "print(words_to_ids['have'])      # 3081\n",
    "print(len(words_to_ids))         # 63,642\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramOne(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, _):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    out = self.ffw(emb)\n",
    "    sft = self.max(out)\n",
    "    return -(sft[0, trgs]).log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTwo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=ctx*voc)\n",
    "    self.max = torch.nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    hid = self.ffw(emb)\n",
    "    lgt = hid.view(self.ctx, -1)\n",
    "    sft = self.max(lgt)\n",
    "    arg = torch.arange(sft.size(0))\n",
    "    foo = sft[arg, trgs]\n",
    "    return -foo.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramTre(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inpt, trgs):\n",
    "    emb = self.emb(inpt)\n",
    "    ctx = self.ffw.weight[trgs]\n",
    "    lgt = torch.mm(ctx, emb.T)\n",
    "    sig = self.sig(lgt)\n",
    "    return -sig.log().mean()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  # def forward(self, inpt, trgs, rand):\n",
    "  #   emb = self.emb(inpt)\n",
    "  #   ctx = self.ffw.weight[trgs]\n",
    "  #   rnd = self.ffw.weight[rand]\n",
    "  #   out = torch.mm(ctx, emb.T)\n",
    "  #   rnd = torch.mm(rnd, emb.T)\n",
    "  #   out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   pst = -out.log().mean()\n",
    "  #   ngt = -(1 - rnd).log().mean()\n",
    "  #   return pst + ngt\n",
    "\n",
    "#new forwarding for batch size \n",
    "  def forward(self, inpt, trgs, rand):\n",
    "    # Embedding lookup for input (shape: [batch_size, embedding_dim])\n",
    "    emb = self.emb(inpt)\n",
    "    \n",
    "    # Ensure context (trgs) and random samples (rand) have the same batch size as inpt\n",
    "    batch_size = inpt.size(0)  # Get the current batch size\n",
    "\n",
    "    # Slice or generate the random tensor according to the input batch size\n",
    "    rand = rand[:batch_size]  # Adjust random tensor to match current batch size\n",
    "    \n",
    "    ctx = self.ffw.weight[trgs.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "    rnd = self.ffw.weight[rand.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "\n",
    "    # Ensure the batch size matches before performing batch matrix multiplication\n",
    "    assert ctx.size(0) == emb.size(0), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    assert rnd.size(0) == emb.size(0), f\"Random batch size {rnd.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    out = torch.bmm(ctx, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    rnd = torch.bmm(rnd, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Apply sigmoid and clamp to prevent NaNs\n",
    "    out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "    rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "    # Calculate loss\n",
    "    pst = -out.log().mean()   # Positive sample log-likelihood\n",
    "    ngt = -(1 - rnd).log().mean()  # Negative sample log-likelihood\n",
    "    \n",
    "    return pst + ngt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "args = (len(words_to_ids), 64, 2)\n",
    "mOne = SkipGramOne(*args)\n",
    "mTwo = SkipGramTwo(*args)\n",
    "mTre = SkipGramTre(*args)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "print('mOne', sum(p.numel() for p in mOne.parameters()))\n",
    "print('mTwo', sum(p.numel() for p in mTwo.parameters()))\n",
    "print('mTre', sum(p.numel() for p in mTre.parameters()))\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "opOne = torch.optim.Adam(mOne.parameters(), lr=0.003)\n",
    "opTwo = torch.optim.Adam(mTwo.parameters(), lr=0.003)\n",
    "opTre = torch.optim.Adam(mTre.parameters(), lr=0.003)\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mOne')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     opOne.zero_grad()\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     loss = mOne(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opOne.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTwo')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(wins, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTwo.zero_grad()\n",
    "#     loss = mTwo(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTwo.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "# #\n",
    "# #\n",
    "# #\n",
    "# wandb.init(project='skip-gram', name='mTre')\n",
    "# for epoch in range(10):\n",
    "#   wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#   prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for i, tks in prgs:\n",
    "#     inpt = torch.LongTensor([tks[1]])\n",
    "#     trgs = torch.LongTensor([tks[0], tks[2]])\n",
    "#     opTre.zero_grad()\n",
    "#     loss = mTre(inpt, trgs)\n",
    "#     loss.backward()\n",
    "#     opTre.step()\n",
    "#     wandb.log({'loss': loss.item()})\n",
    "# wandb.finish()\n",
    "import torch\n",
    "import more_itertools\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"cbow_training\", entity=\"omareweis123\", name='batch_size(4096),tokens1000000000,30epochs,titlesadded')\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 4096\n",
    "learning_rate = 0.001  # Define your learning rate\n",
    "mFoo = mFoo.to(device)\n",
    "\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Initialize the optimizer\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    wins = list(more_itertools.windowed(tokens[:1000000000], window_size))  # Convert to list for easier batching\n",
    "    prgs = tqdm.tqdm(range(0, len(wins), batch_size), total=len(wins) // batch_size, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "    num_batches = 0   # Counter for batches\n",
    "\n",
    "    for batch_idx in prgs:\n",
    "        batch_wins = wins[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Prepare batch inputs and targets, ensuring they're on the correct device\n",
    "        inpts = torch.LongTensor([win[context_size] for win in batch_wins]).to(device)  # Central token for each window\n",
    "        trgs = torch.LongTensor([[win[i] for i in range(context_size)] + [win[i] for i in range(context_size + 1, window_size)]\n",
    "                                  for win in batch_wins]).to(device)  # Context tokens (left and right)\n",
    "        rand = torch.randint(0, len(words_to_ids), (batch_size, 2)).to(device)  # Random negative samples on the same device\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = mFoo(inpts, trgs, rand)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item(), 'learning_rate': learning_rate})\n",
    "\n",
    "    # Calculate and log average loss for the epoch\n",
    "    average_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    wandb.log({'average_loss': average_loss})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## this is without batch size \n",
    "# # Initialize Weights and Biases\n",
    "# wandb.init(project=\"cbow_training\", entity=\"omareweis123\",name='clamped_logs')\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# mFoo = mFoo.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(10):\n",
    "#     wins = more_itertools.windowed(tokens[:10000], 3)\n",
    "#     prgs = tqdm.tqdm(enumerate(wins), total=len(tokens[:10000]), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#     for i, tks in prgs:\n",
    "#         # Move input tensors to the same device (GPU or CPU)\n",
    "#         inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#         trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#         rand = torch.randint(0, len(words_to_ids), (2,)).to(device)\n",
    "\n",
    "#         # Zero gradients\n",
    "#         opFoo.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         loss = mFoo(inpt, trgs, rand)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         opFoo.step()\n",
    "\n",
    "#         # Log the loss\n",
    "#         wandb.log({'loss': loss.item()})\n",
    "\n",
    "# # Finish the W&B logging\n",
    "# wandb.finish()\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# wandb.init(project=\"cbow_training\", entity=\"omareweis123\")\n",
    "\n",
    "# # Define your token limit here (e.g., 10,000 tokens)\n",
    "# token_limit = 10000\n",
    "\n",
    "# # Split data into 80% train, 20% validation from the limited set\n",
    "# train_tokens = tokens[:int(0.8 * token_limit)]\n",
    "# val_tokens = tokens[int(0.8 * token_limit):token_limit]\n",
    "\n",
    "# # Move the model to the correct device\n",
    "# mFoo = mFoo.to(device)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     # Training loop\n",
    "#     mFoo.train()\n",
    "#     train_wins = list(more_itertools.windowed(train_tokens, 3))  # Create training windows\n",
    "#     train_prgs = tqdm.tqdm(enumerate(train_wins), total=len(train_wins), desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "#     train_loss_total = 0\n",
    "#     for i, tks in train_prgs:\n",
    "#         if None in tks:  # Skip invalid windows (e.g., at the end)\n",
    "#             continue\n",
    "#         inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#         trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#         rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#         opFoo.zero_grad()\n",
    "#         loss = mFoo(inpt, trgs, rand)\n",
    "#         loss.backward()\n",
    "#         opFoo.step()\n",
    "#         train_loss_total += loss.item()\n",
    "#         wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "#     avg_train_loss = train_loss_total / len(train_wins)\n",
    "#     wandb.log({'avg_train_loss': avg_train_loss})\n",
    "\n",
    "#     # Validation loop\n",
    "#     mFoo.eval()\n",
    "#     val_wins = list(more_itertools.windowed(val_tokens, 3))  # Create validation windows\n",
    "#     val_prgs = tqdm.tqdm(enumerate(val_wins), total=len(val_wins), desc=f\"Epoch {epoch+1} [Validation]\", leave=False)\n",
    "#     val_loss_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, tks in val_prgs:\n",
    "#             if None in tks:  # Skip invalid windows\n",
    "#                 continue\n",
    "#             inpt = torch.LongTensor([tks[1]]).to(device)\n",
    "#             trgs = torch.LongTensor([tks[0], tks[2]]).to(device)\n",
    "#             rand = torch.randint(0, len(words_to_ids), (2,)).to(device)  # Move rand to device\n",
    "#             val_loss = mFoo(inpt, trgs, rand)\n",
    "#             val_loss_total += val_loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss_total / len(val_wins)\n",
    "#     wandb.log({'avg_val_loss': avg_val_loss})\n",
    "\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ids_to_word dictionary: 70572\n"
     ]
    }
   ],
   "source": [
    "size_of_ids_to_word = len(ids_to_words)\n",
    "print(f\"Size of ids_to_word dictionary: {size_of_ids_to_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116327976"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming text8 is your string\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mte\u001b[49m\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Tokenize by splitting on whitespace\u001b[39;00m\n\u001b[0;32m      3\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)  \u001b[38;5;66;03m# Count the number of tokens\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens in text8: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming text8 is your string\n",
    "tokens = te.split()  # Tokenize by splitting on whitespace\n",
    "number_of_tokens = len(tokens)  # Count the number of tokens\n",
    "\n",
    "print(f\"Number of tokens in text8: {number_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "#saving dictionary of words \n",
    "import pickle\n",
    "\n",
    "\n",
    "# save dictionary to person_data.pkl file\n",
    "with open('vocab_dict.pkl', 'wb') as fp:\n",
    "    pickle.dump(ids_to_words, fp)\n",
    "    print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'emb.weight': tensor([[ 1.8787, -0.1666, -1.8597,  ..., -1.0010, -0.9638, -0.6151],\n",
      "        [-0.1616,  0.0884, -0.0934,  ...,  0.0709,  0.4607,  0.6704],\n",
      "        [-0.0627, -0.1363,  0.0163,  ...,  0.0908, -0.1663,  0.4475],\n",
      "        ...,\n",
      "        [-3.3083, -1.5824,  1.5640,  ...,  0.6715,  0.5412, -0.2053],\n",
      "        [-0.5803, -1.9302, -0.1088,  ..., -0.6040, -0.7218,  1.2112],\n",
      "        [-0.4179,  0.4456, -0.5507,  ...,  1.5949,  1.3342, -0.4006]],\n",
      "       device='cuda:0'), 'ffw.weight': tensor([[-0.0559, -3.1326,  3.4805,  ..., -2.4313,  0.8235,  1.0211],\n",
      "        [-0.0750,  4.2574, -4.5288,  ...,  2.9554, -1.2074, -0.9694],\n",
      "        [-0.1841,  3.7869, -4.1561,  ...,  2.5711, -1.0139, -1.0283],\n",
      "        ...,\n",
      "        [-0.1226, -3.0458,  3.6798,  ..., -2.2324,  0.8989,  1.1990],\n",
      "        [-0.1888, -3.4186,  3.6209,  ..., -2.3780,  1.0202,  0.9866],\n",
      "        [-0.1028, -3.1571,  3.4493,  ..., -2.2879,  1.0355,  0.8634]],\n",
      "       device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "#verigying model \n",
    "# Print all weights and biases of the model\n",
    "print(mFoo.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"skipgram_model_titles.pth\"  # You can name the file as you like\n",
    "torch.save(mFoo.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_8700\\321600620.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SkipGramFoo:\n\tsize mismatch for emb.weight: copying a param with shape torch.Size([63642, 64]) from checkpoint, the shape in current model is torch.Size([110812, 64]).\n\tsize mismatch for ffw.weight: copying a param with shape torch.Size([63642, 64]) from checkpoint, the shape in current model is torch.Size([110812, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Load the modified state_dict into the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmFoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m## Set parameters\u001b[39;00m\n\u001b[0;32m     38\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SkipGramFoo:\n\tsize mismatch for emb.weight: copying a param with shape torch.Size([63642, 64]) from checkpoint, the shape in current model is torch.Size([110812, 64]).\n\tsize mismatch for ffw.weight: copying a param with shape torch.Size([63642, 64]) from checkpoint, the shape in current model is torch.Size([110812, 64])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import more_itertools\n",
    "\n",
    "# Step 1: Load the updated dictionary (vocabulary)\n",
    "with open('updated_dict_20.pkl', 'rb') as fp:\n",
    "    updated_vocab = pickle.load(fp)\n",
    "\n",
    "# Step 2: Load the CSV file and preprocess the 'title_cleaned' column\n",
    "df = pd.read_csv('titles_20')\n",
    "titles = df['title_cleaned'].tolist()\n",
    "\n",
    "# Step 3: Preprocess and tokenize the 'title_cleaned' column\n",
    "def tokenize_titles(titles, vocab):\n",
    "    tokens = []\n",
    "    for title in titles:\n",
    "        words = title.split()\n",
    "        tokenized = [vocab.get(word, vocab.get('<UNK>')) for word in words]  # Use <UNK> for unknown words\n",
    "        tokens.append(tokenized)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the new data\n",
    "tokenized_titles = tokenize_titles(titles, updated_vocab)\n",
    "\n",
    "# Step 4: Load the trained model\n",
    "model_path = \"skipgram_model.pth\"  # Update with your actual saved model path\n",
    "mFoo = SkipGramFoo(len(updated_vocab), 64, 2).to(device)\n",
    "\n",
    "# Step 5: Load the state_dict and modify keys\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "mFoo.load_state_dict(state_dict)\n",
    "\n",
    "## Set parameters\n",
    "batch_size = 512\n",
    "learning_rate = 0.001  # Define your learning rate\n",
    "mFoo = mFoo.to(device)\n",
    "\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Initialize the optimizer\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    wins = list(more_itertools.windowed(tokenized_titles[:1000000000], window_size))  # Convert to list for easier batching\n",
    "    prgs = tqdm.tqdm(range(0, len(wins), batch_size), total=len(wins) // batch_size, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "    num_batches = 0   # Counter for batches\n",
    "\n",
    "    for batch_idx in prgs:\n",
    "        batch_wins = wins[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Prepare batch inputs and targets, ensuring they're on the correct device\n",
    "        inpts = torch.LongTensor([win[context_size] for win in batch_wins]).to(device)  # Central token for each window\n",
    "        trgs = torch.LongTensor([[win[i] for i in range(context_size)] + [win[i] for i in range(context_size + 1, window_size)]\n",
    "                                  for win in batch_wins]).to(device)  # Context tokens (left and right)\n",
    "        rand = torch.randint(0, len(updated_vocab), (batch_size, 2)).to(device)  # Random negative samples on the same device\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = mFoo(inpts, trgs, rand)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item(), 'learning_rate': learning_rate})\n",
    "\n",
    "    # Calculate and log average loss for the epoch\n",
    "    average_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    wandb.log({'average_loss': average_loss})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'anarchism':\n",
      "anarchism: 1.0000\n",
      "anarcho: 0.6730\n",
      "libertarianism: 0.6398\n",
      "capitalism: 0.6392\n",
      "conservatism: 0.6252\n",
      "individualist: 0.6215\n",
      "ideologies: 0.6089\n",
      "sombart: 0.5846\n",
      "darwinism: 0.5809\n",
      "critique: 0.5808\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'anarchism'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Initialize a list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Loop through all words in your vocabulary to calculate cosine similarity\n",
    "for idx in range(len(words_to_ids)):\n",
    "    # Get the embedding for each word\n",
    "    current_embedding = mFoo.emb(torch.LongTensor([idx]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(target_embedding, current_embedding)  # Shape: [1]\n",
    "    similarities.append((ids_to_words[idx], similarity.item()))  # Store the word and its similarity\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities[:10]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words similar to '{}':\".format(word_to_check))\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'car':\n",
      "car: 1.0000\n",
      "cars: 0.7031\n",
      "passenger: 0.5895\n",
      "airliners: 0.5852\n",
      "canaveral: 0.5777\n",
      "diesel: 0.5452\n",
      "automobile: 0.5444\n",
      "boat: 0.5436\n",
      "transatlantic: 0.5403\n",
      "supersonic: 0.5341\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'car'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the target word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Get all embeddings at once and move to GPU\n",
    "with torch.no_grad():\n",
    "    all_embeddings = mFoo.emb.weight.to(device)  # Shape: [vocab_size, embedding_dim]\n",
    "\n",
    "# Repeat the target embedding to match the shape of all embeddings\n",
    "target_embedding = target_embedding.expand_as(all_embeddings)  # Shape: [vocab_size, embedding_dim]\n",
    "\n",
    "# Compute cosine similarity in one batch\n",
    "with torch.no_grad():\n",
    "    similarities = cosine_similarity(target_embedding, all_embeddings)  # Shape: [vocab_size]\n",
    "\n",
    "# Convert similarities to a list of tuples with word and similarity score\n",
    "similarities_list = [(ids_to_words[idx], similarities[idx].item()) for idx in range(len(words_to_ids))]\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities_list[:10]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Top 10 words similar to '{word_to_check}':\")\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'france':\n",
      "france: 1.0000\n",
      "italy: 0.7116\n",
      "french: 0.6191\n",
      "spain: 0.6022\n",
      "luxembourg: 0.5642\n",
      "maggiore: 0.5628\n",
      "paris: 0.5480\n",
      "normandy: 0.5472\n",
      "belgium: 0.5318\n",
      "monaco: 0.5258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'france'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Initialize a list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Loop through all words in your vocabulary to calculate cosine similarity\n",
    "for idx in range(len(words_to_ids)):\n",
    "    # Get the embedding for each word\n",
    "    current_embedding = mFoo.emb(torch.LongTensor([idx]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(target_embedding, current_embedding)  # Shape: [1]\n",
    "    similarities.append((ids_to_words[idx], similarity.item()))  # Store the word and its similarity\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities[:10]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words similar to '{}':\".format(word_to_check))\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'fruit':\n",
      "fruit: 1.0000\n",
      "meat: 0.7175\n",
      "potatoes: 0.6929\n",
      "maize: 0.6673\n",
      "vegetables: 0.6496\n",
      "tomatoes: 0.6483\n",
      "goats: 0.6483\n",
      "foods: 0.6424\n",
      "boiled: 0.6423\n",
      "chocolate: 0.6352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have the model and vocab already set up\n",
    "word_to_check = 'fruit'  # The word you want to check similarity for\n",
    "word_index = words_to_ids[word_to_check]  # Get the index of the word\n",
    "embedding_dim = 64  # Set the embedding dimension used in your model\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "# Get the embedding for the word you want to check\n",
    "with torch.no_grad():\n",
    "    target_embedding = mFoo.emb(torch.LongTensor([word_index]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "# Initialize a list to store similarities\n",
    "similarities = []\n",
    "\n",
    "# Loop through all words in your vocabulary to calculate cosine similarity\n",
    "for idx in range(len(words_to_ids)):\n",
    "    # Get the embedding for each word\n",
    "    current_embedding = mFoo.emb(torch.LongTensor([idx]).to(device))  # Shape: [1, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(target_embedding, current_embedding)  # Shape: [1]\n",
    "    similarities.append((ids_to_words[idx], similarity.item()))  # Store the word and its similarity\n",
    "\n",
    "# Sort by similarity score\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 most similar words\n",
    "top_similar_words = similarities[:10]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words similar to '{}':\".format(word_to_check))\n",
    "for word, sim in top_similar_words:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
