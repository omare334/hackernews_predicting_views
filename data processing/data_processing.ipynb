{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toeknising the titles \n",
    "We will now decapatilise and tokenise(segment) the titles so that we can process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing df of story rows rather that comments \n",
    "df = pd.read_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0          \"What May Happen in the Next Hundred Years\", f...\n",
       "1               Getting Started with JavaScript Unit Testing\n",
       "2          Armstrong, the Django-based and open-source ne...\n",
       "3                       Why Web Reviewers Make Up Bad Things\n",
       "4          You Weren't Meant to Have a Boss: The Cliff Notes\n",
       "                                 ...                        \n",
       "5351743    Scrapling: Fast, Adaptive Web Scraping for Python\n",
       "5351744                         Monkeys Predict US Elections\n",
       "5351745                                                  NaN\n",
       "5351746                                                  NaN\n",
       "5351747                                                  NaN\n",
       "Name: title, Length: 5351748, dtype: object>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \n",
      "0     \"what may happen in the next hundred years\", f...         19.0  \n",
      "1          getting started with javascript unit testing          0.0  \n",
      "2     armstrong, the django-based and open-source ne...          0.0  \n",
      "3                  why web reviewers make up bad things          0.0  \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0  \n",
      "...                                                 ...          ...  \n",
      "2638  vmware communities roundtable podcast - show n...          0.0  \n",
      "2639                                     teeth problems         -1.0  \n",
      "2640  microsoft and nbc call it quits, for real: her...         -1.0  \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0  \n",
      "2642           microsoft hits back as google muscles in          1.0  \n",
      "\n",
      "[2123 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Set seed for consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Function to check if the title is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False  # If detection fails, we assume it's not English\n",
    "\n",
    "# Filter rows where the title is in English\n",
    "df['title'] = df['title'].str.lower()\n",
    "df = df[df['title'].apply(is_english)]\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \n",
      "0     \"what may happen in the next hundred years\", f...         19.0  \n",
      "1          getting started with javascript unit testing          0.0  \n",
      "2     armstrong, the django-based and open-source ne...          0.0  \n",
      "3                  why web reviewers make up bad things          0.0  \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0  \n",
      "...                                                 ...          ...  \n",
      "2638  vmware communities roundtable podcast - show n...          0.0  \n",
      "2639                                     teeth problems         -1.0  \n",
      "2640  microsoft and nbc call it quits, for real: her...         -1.0  \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0  \n",
      "2642           microsoft hits back as google muscles in          1.0  \n",
      "\n",
      "[2123 rows x 12 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources if you haven't done so already\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function to tokenize and apply stemming/lemmatization\n",
    "#def tokenize_and_process(text):\n",
    "    #if not isinstance(text, str) or not text:  # Check if the text is a valid non-empty string\n",
    "        #return ''  # Return empty string for non-string or empty values\n",
    "\n",
    "    # Tokenize the text by whitespace\n",
    "    #tokens = text.split()\n",
    "    \n",
    "    # Apply stemming and lemmatization\n",
    "    #processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    \n",
    "    #return ' '.join(processed_tokens)  # Return the processed tokens as a single string\n",
    "\n",
    "# Function to tokenize without stemming/lemmatization\n",
    "def tokenize_and_process(text):\n",
    "    if not isinstance(text, str) or not text:  # Check if the text is a valid non-empty string\n",
    "        return ''  # Return empty string for non-string or empty values\n",
    "\n",
    "    # Tokenize the text by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return ' '.join(tokens)  # Return the tokens as a single string\n",
    "\n",
    "\n",
    "# Apply the function to create the 'adjusted_title' column\n",
    "df['title'] = df['title'].apply(tokenize_and_process)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \\\n",
      "0     \"what may happen in the next hundred years\"<CO...         19.0   \n",
      "1          getting started with javascript unit testing          0.0   \n",
      "2     armstrong<COMMA> the django-based and open-sou...          0.0   \n",
      "3                  why web reviewers make up bad things          0.0   \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0   \n",
      "...                                                 ...          ...   \n",
      "2638  vmware communities roundtable podcast - show n...          0.0   \n",
      "2639                                     teeth problems         -1.0   \n",
      "2640  microsoft and nbc call it quits<COMMA> for rea...         -1.0   \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0   \n",
      "2642           microsoft hits back as google muscles in          1.0   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [\"what, may, happen, in, the, next, hundred, y...   \n",
      "1     [getting, started, with, javascript, unit, tes...   \n",
      "2     [armstrong<COMMA>, the, django-based, and, ope...   \n",
      "3          [why, web, reviewers, make, up, bad, things]   \n",
      "4     [you, weren't, meant, to, have, a, boss:, the,...   \n",
      "...                                                 ...   \n",
      "2638  [vmware, communities, roundtable, podcast, -, ...   \n",
      "2639                                  [teeth, problems]   \n",
      "2640  [microsoft, and, nbc, call, it, quits<COMMA>, ...   \n",
      "2641  [woodstock, insulated, siding, |, james, hardi...   \n",
      "2642   [microsoft, hits, back, as, google, muscles, in]   \n",
      "\n",
      "                                              token_ids  \n",
      "0     [45, 3573, 2668, 2929, 5549, 3868, 2839, 6223,...  \n",
      "1                  [2502, 5277, 6130, 3150, 5813, 5537]  \n",
      "2     [705, 5549, 1820, 620, 4003, 3857, 1312, 3106,...  \n",
      "3             [6085, 6034, 4731, 3509, 5833, 809, 5572]  \n",
      "4     [6232, 6062, 3591, 5626, 2697, 429, 1002, 5549...  \n",
      "...                                                 ...  \n",
      "2638  [5956, 1378, 4800, 4290, 203, 5063, 3923, 51, ...  \n",
      "2639                                       [5513, 4402]  \n",
      "2640  [3652, 620, 3828, 1112, 3114, 4520, 2342, 4585...  \n",
      "2641  [6142, 3015, 5075, 6266, 3137, 2679, 5075, 626...  \n",
      "2642           [3652, 2769, 801, 717, 2554, 3780, 2929]  \n",
      "\n",
      "[2123 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to replace punctuation with special tokens\n",
    "def preprocess_text(text):\n",
    "    # Replace specific punctuation with special tokens\n",
    "    text = text.replace(',', '<COMMA>')  # Replace commas\n",
    "    text = text.replace('.', '<FULLSTOP>')  # Replace full stops\n",
    "    text = text.replace('!', '<EXCLAMATION>')  # Replace exclamations\n",
    "    return text\n",
    "\n",
    "# Tokenize the text (split into individual words and special tokens)\n",
    "def tokenize_text(text):\n",
    "    # Tokenize by splitting on whitespace and keeping punctuation tokens\n",
    "    return re.findall(r'\\S+|<COMMA>|<FULLSTOP>|<EXCLAMATION>', text)\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['tokens'] = df['title'].apply(tokenize_text)\n",
    "\n",
    "# Create a vocabulary (a dictionary mapping tokens to unique IDs)\n",
    "def create_vocabulary(tokens_series):\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "    # Create a unique token list and assign an ID to each\n",
    "    vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "    return vocabulary\n",
    "\n",
    "# Get vocabulary from the tokens in the DataFrame\n",
    "vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# Function to convert tokens to token IDs using the vocabulary\n",
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "# Apply the function to convert tokens to token IDs\n",
    "df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# Display the DataFrame with token IDs\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \\\n",
      "0     <DOUBLEQUOTE>what may happen in the next hundr...         19.0   \n",
      "1          getting started with javascript unit testing          0.0   \n",
      "2     armstrong<COMMA> the django<DASH>based and ope...          0.0   \n",
      "3                  why web reviewers make up bad things          0.0   \n",
      "4     you weren<QUOTE>t meant to have a boss<COLON> ...          1.0   \n",
      "...                                                 ...          ...   \n",
      "2638  vmware communities roundtable podcast <DASH> s...          0.0   \n",
      "2639                                     teeth problems         -1.0   \n",
      "2640  microsoft and nbc call it quits<COMMA> for rea...         -1.0   \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0   \n",
      "2642           microsoft hits back as google muscles in          1.0   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [<DOUBLEQUOTE>what, may, happen, in, the, next...   \n",
      "1     [getting, started, with, javascript, unit, tes...   \n",
      "2     [armstrong<COMMA>, the, django<DASH>based, and...   \n",
      "3          [why, web, reviewers, make, up, bad, things]   \n",
      "4     [you, weren<QUOTE>t, meant, to, have, a, boss<...   \n",
      "...                                                 ...   \n",
      "2638  [vmware, communities, roundtable, podcast, <DA...   \n",
      "2639                                  [teeth, problems]   \n",
      "2640  [microsoft, and, nbc, call, it, quits<COMMA>, ...   \n",
      "2641  [woodstock, insulated, siding, |, james, hardi...   \n",
      "2642   [microsoft, hits, back, as, google, muscles, in]   \n",
      "\n",
      "                                              token_ids  \n",
      "0     [302, 3575, 2670, 2931, 5551, 3870, 2841, 6225...  \n",
      "1                  [2504, 5279, 6132, 3152, 5815, 5539]  \n",
      "2     [708, 5551, 1822, 623, 4005, 3859, 1314, 3108,...  \n",
      "3             [6087, 6036, 4733, 3511, 5835, 811, 5574]  \n",
      "4     [6234, 6064, 3593, 5628, 2699, 431, 1004, 5551...  \n",
      "...                                                 ...  \n",
      "2638  [5958, 1380, 4802, 4292, 256, 5065, 3925, 3, 2...  \n",
      "2639                                       [5515, 4405]  \n",
      "2640  [3654, 623, 3830, 1114, 3116, 4522, 2344, 4586...  \n",
      "2641  [6144, 3017, 5077, 6267, 3139, 2681, 5077, 626...  \n",
      "2642           [3654, 2771, 803, 720, 2556, 3782, 2931]  \n",
      "\n",
      "[2123 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to replace all punctuation with special tokens\n",
    "def preprocess_text(text):\n",
    "    # Replace various punctuation with special tokens\n",
    "    replacements = {\n",
    "        ',': '<COMMA>',\n",
    "        '.': '<FULLSTOP>',\n",
    "        '!': '<EXCLAMATION>',\n",
    "        '?': '<QUESTION>',\n",
    "        ';': '<SEMICOLON>',\n",
    "        ':': '<COLON>',\n",
    "        '(': '<OPENPAREN>',\n",
    "        ')': '<CLOSEPAREN>',\n",
    "        '[': '<OPENBRACKET>',\n",
    "        ']': '<CLOSEBRACKET>',\n",
    "        '{': '<OPENBRACE>',\n",
    "        '}': '<CLOSEBRACE>',\n",
    "        '\\'': '<QUOTE>',\n",
    "        '\\\"': '<DOUBLEQUOTE>',\n",
    "        '-': '<DASH>',\n",
    "        '_': '<UNDERLINE>',\n",
    "        '…': '<ELLIPSIS>'  # Ellipsis\n",
    "    }\n",
    "    \n",
    "    for punctuation, token in replacements.items():\n",
    "        text = text.replace(punctuation, token)  # Replace each punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize the text (split into individual words and special tokens)\n",
    "def tokenize_text(text):\n",
    "    # Tokenize by splitting on whitespace and keeping punctuation tokens\n",
    "    return re.findall(r'\\S+|<COMMA>|<FULLSTOP>|<EXCLAMATION>|<QUESTION>|<SEMICOLON>|<COLON>|<OPENPAREN>|<CLOSEPAREN>|<OPENBRACKET>|<CLOSEBRACKET>|<OPENBRACE>|<CLOSEBRACE>|<QUOTE>|<DOUBLEQUOTE>|<DASH>|<UNDERLINE>|<ELLIPSIS>', text)\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['tokens'] = df['title'].apply(tokenize_text)\n",
    "\n",
    "# Create a vocabulary (a dictionary mapping tokens to unique IDs)\n",
    "def create_vocabulary(tokens_series):\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "    # Create a unique token list and assign an ID to each\n",
    "    vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "    return vocabulary\n",
    "\n",
    "# Get vocabulary from the tokens in the DataFrame\n",
    "vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# Function to convert tokens to token IDs using the vocabulary\n",
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "# Apply the function to convert tokens to token IDs\n",
    "df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# Display the DataFrame with token IDs\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pandas as pd\n",
    "#import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Step 1: Preprocess the text (optional)\n",
    "# def preprocess_text(text):\n",
    "#     # Replace specific punctuation with special tokens\n",
    "#     text = text.replace(',', '<COMMA>')  # Replace commas\n",
    "#     text = text.replace('.', '<FULLSTOP>')  # Replace full stops\n",
    "#     text = text.replace('!', '<EXCLAMATION>')  # Replace exclamations\n",
    "#     return text\n",
    "\n",
    "# # Preprocess the 'title' column\n",
    "# df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# # Step 2: Save titles to a file for training SentencePiece\n",
    "# with open('titles.txt', 'w') as f:\n",
    "#     for title in df['title']:\n",
    "#         f.write(title + '\\n')\n",
    "\n",
    "# # Step 3: Train a SentencePiece model\n",
    "# # Increase vocab_size to at least 100 to avoid the error\n",
    "# spm.SentencePieceTrainer.train('--input=titles.txt --model_prefix=m --vocab_size=100 --character_coverage=1.0')\n",
    "\n",
    "# # Step 4: Load the trained SentencePiece model\n",
    "# sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# # Tokenize the 'title' column using SentencePiece\n",
    "# df['tokens'] = df['title'].apply(lambda x: sp.encode(x, out_type=str))\n",
    "\n",
    "# # Create a vocabulary from the tokens\n",
    "# def create_vocabulary(tokens_series):\n",
    "#     all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "#     vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "#     return vocabulary\n",
    "\n",
    "# # Get vocabulary from the tokens in the DataFrame\n",
    "# vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# # Function to convert tokens to token IDs using the vocabulary\n",
    "# def tokens_to_ids(tokens, vocab):\n",
    "#     return [vocab[token] for token in tokens]\n",
    "\n",
    "# # Apply the function to convert tokens to token IDs\n",
    "# df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# # Display the DataFrame with tokens and token IDs\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding \n",
    "\n",
    "We now need to get the embedding first we will analyze the embedding process for the wikipedia text before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the required NLTK resources if you haven't already\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace specific punctuation with special tokens\n",
    "    replacements = {\n",
    "        ',': '<COMMA>',\n",
    "        '.': '<FULLSTOP>',\n",
    "        '!': '<EXCLAMATION>',\n",
    "        '?': '<QUESTION>',\n",
    "        ';': '<SEMICOLON>',\n",
    "        ':': '<COLON>',\n",
    "        '(': '<OPENPAREN>',\n",
    "        ')': '<CLOSEPAREN>',\n",
    "        '[': '<OPENBRACKET>',\n",
    "        ']': '<CLOSEBRACKET>',\n",
    "        '{': '<OPENBRACE>',\n",
    "        '}': '<CLOSEBRACE>',\n",
    "        '\\'': '<QUOTE>',\n",
    "        '\\\"': '<DOUBLEQUOTE>',\n",
    "        '-': '<DASH>',\n",
    "        '_': '<UNDERLINE>',\n",
    "        '…': '<ELLIPSIS>'  # Ellipsis\n",
    "    }\n",
    "\n",
    "    # Replace punctuation with special tokens\n",
    "    for punctuation, token in replacements.items():\n",
    "        text = text.replace(punctuation, token)\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to tokenize and apply stemming/lemmatization\n",
    "#def tokenize_and_process(text):\n",
    "    # Tokenize the text by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Function to tokenize without stemming/lemmatization\n",
    "def tokenize_and_process(text):\n",
    "    # Tokenize the text by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "file_path = 'text8.txt'  # Replace with your file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Preprocess the content\n",
    "preprocessed_content = preprocess_text(content)\n",
    "\n",
    "# Tokenize and process the preprocessed text\n",
    "tokens = tokenize_and_process(preprocessed_content)\n",
    "\n",
    "# Create sentences for Word2Vec\n",
    "sentences = [tokens]  # Wrap tokens in a list to form a single sentence\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.2169328e-03  8.1457589e-03  2.5015855e-03  4.4307020e-03\n",
      " -4.2153504e-03 -8.9858631e-03 -1.5310884e-03  1.3215863e-03\n",
      " -8.2708237e-04 -3.5400926e-03 -6.4976024e-03  2.7700793e-03\n",
      " -6.0778796e-03  6.4819301e-03 -6.1873733e-03  6.1717201e-03\n",
      "  4.2399121e-03 -8.8933287e-03  3.0457056e-03 -6.9996440e-03\n",
      "  2.3178649e-03 -5.5011176e-03 -3.6389255e-03  1.3364435e-03\n",
      "  6.2675681e-03  1.7851364e-03  2.8404749e-03 -4.4528805e-03\n",
      " -4.4578002e-03  8.7969489e-03  5.5197608e-03 -3.6655618e-03\n",
      "  2.6506258e-03 -4.0250420e-04  1.9467354e-04  9.6652545e-03\n",
      "  5.5776238e-03 -1.6557014e-03  4.7752871e-03 -5.9111891e-03\n",
      "  8.1074676e-03 -2.9322363e-03 -5.8023944e-03 -3.9075706e-03\n",
      "  5.9711919e-03  9.1314232e-03 -3.7671484e-03  3.9444994e-03\n",
      "  8.0300914e-03 -6.0865879e-03  2.4882329e-03 -7.5024664e-03\n",
      " -6.8915915e-03 -5.3569768e-03  6.8802941e-03  9.0985297e-05\n",
      "  9.9987574e-03 -9.4637321e-03  3.0739342e-03 -4.0175333e-03\n",
      " -9.6577499e-03 -3.0278838e-03 -2.1026398e-03 -4.0729572e-03\n",
      "  4.1171825e-03  5.9761559e-03 -2.2428989e-04  8.9856088e-03\n",
      "  4.6625827e-03  2.5806702e-03 -5.9624375e-03 -5.5925251e-04\n",
      "  8.9123640e-03 -9.5521333e-03 -2.2385847e-03  8.1666755e-03\n",
      "  9.4301226e-03 -5.8281421e-05  8.7142584e-04  1.3335752e-03\n",
      "  8.7805307e-03 -6.0989023e-03  1.2571693e-03  3.6974335e-03\n",
      "  5.4186536e-03  3.5776638e-03  1.2447607e-03 -4.9016844e-03\n",
      "  9.2381863e-03 -2.0283889e-03  4.2015123e-03  5.1314463e-03\n",
      "  5.6766295e-03  6.2200427e-04  1.1027587e-03 -5.5785491e-03\n",
      "  9.0440251e-03  3.1770588e-04 -6.9266688e-03 -5.7845758e-03]\n"
     ]
    }
   ],
   "source": [
    "# Example usage: getting the vector for a word\n",
    "word_vector = model.wv['exampl']  # Replace 'example' with any word from your corpus\n",
    "\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: -0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your pre-trained Word2Vec model or train one\n",
    "# model = Word2Vec.load(\"your_model_path\")  # Load an existing model\n",
    "# Or train your model\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example word pairs and their human-annotated similarity scores\n",
    "word_pairs = [('king', 'queen'), ('man', 'woman')]  # Example pairs\n",
    "human_scores = [0.8, 0.9]  # Human-annotated scores\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = []\n",
    "for w1, w2 in word_pairs:\n",
    "    try:\n",
    "        vec1 = model.wv[w1]  # Get the vector for the first word\n",
    "        vec2 = model.wv[w2]  # Get the vector for the second word\n",
    "        cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        similarities.append(cosine_similarity)\n",
    "    except KeyError as e:\n",
    "        print(f\"Word not in vocabulary: {e}\")\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "if similarities:  # Ensure there are similarities to compare\n",
    "    corr, _ = spearmanr(human_scores, similarities)\n",
    "    print(f'Spearman correlation: {corr}')\n",
    "else:\n",
    "    print(\"No valid similarities calculated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vectors for the first row tokens:\n",
      "Token: the, Vector: [ 0.00215919 -0.00915128  0.00352259 -0.00158813 -0.00605567 -0.00716093\n",
      " -0.00900247  0.00796636 -0.00185247  0.00825693 -0.00636896 -0.00516181\n",
      " -0.00809662  0.00962218  0.00150697 -0.00340323 -0.00037912 -0.00676415\n",
      "  0.00090843  0.00869125 -0.00113445 -0.0036642   0.00731904  0.00285193\n",
      " -0.00022883  0.00754815 -0.00137701 -0.00993149 -0.0034574   0.00358534\n",
      " -0.00541263 -0.00361407 -0.00507     0.00108166  0.00853553 -0.00947306\n",
      "  0.00770152  0.0029809   0.00277802 -0.00991549  0.00115307 -0.00178094\n",
      " -0.00226703  0.00141892 -0.00702713 -0.0025684  -0.00595684  0.00045144\n",
      "  0.00494801 -0.00108522  0.00096037 -0.00095726 -0.00519448 -0.00044592\n",
      " -0.0070453   0.00470832 -0.00162558 -0.00965235  0.00043421  0.00440209\n",
      "  0.00909637 -0.00408069  0.00030179  0.00495305 -0.00971529  0.00876166\n",
      "  0.00380845 -0.00994397  0.00737749 -0.00076256 -0.00283092 -0.00537754\n",
      "  0.00346894 -0.00313984 -0.00986447 -0.00942327 -0.00095738  0.00299414\n",
      " -0.00444075 -0.00551423 -0.00062515 -0.00520914 -0.00251135  0.00584483\n",
      "  0.00729718  0.00266109  0.00431992  0.00137032 -0.00893584 -0.00981578\n",
      "  0.00276528  0.00890743 -0.00572966 -0.00285647  0.00523159  0.00418457\n",
      " -0.00611716 -0.00720104 -0.00520399  0.00414936]\n",
      "Token: night, Vector: [-0.00385531  0.03280445 -0.00944056 -0.04038892  0.01440596 -0.06678791\n",
      "  0.03364242  0.09118249 -0.04176325 -0.0495032  -0.03758153 -0.01493819\n",
      " -0.0392101   0.01223121 -0.01435482 -0.04523433  0.0418596  -0.01187859\n",
      " -0.01955666 -0.08018895  0.07562888 -0.00277904  0.05190802 -0.06419507\n",
      "  0.05786058 -0.07276183 -0.01774934 -0.06154617 -0.0164464   0.03352268\n",
      "  0.04351642  0.01857965  0.03428493 -0.11767921  0.01256722  0.02262602\n",
      " -0.02369316  0.01221754 -0.02288104 -0.0912477  -0.03090997 -0.04291422\n",
      "  0.04118691  0.00918542  0.06259588 -0.02158185 -0.01421977 -0.05601916\n",
      "  0.02163629  0.00562336 -0.00414908 -0.06530441  0.01526206 -0.01150012\n",
      " -0.05402902  0.00445401  0.01089819 -0.01022171 -0.06500185  0.02020744\n",
      "  0.04617047 -0.03057749  0.04020904  0.00431711 -0.04521146  0.04549045\n",
      "  0.0454453   0.10011107 -0.10740501  0.13345602 -0.03870368  0.05460679\n",
      "  0.03021699 -0.05118608  0.06136799  0.01117864  0.00915822  0.04814783\n",
      " -0.01081689 -0.02919845 -0.04625767 -0.01146097 -0.05900848  0.0675756\n",
      " -0.00424401 -0.07532721  0.07493955  0.03455284  0.01025738  0.07521048\n",
      "  0.02664559  0.05658451 -0.01988294 -0.02241144  0.08521339 -0.02692281\n",
      "  0.02446247 -0.05768365  0.02990625  0.04861479]\n",
      "Token: i, Vector: [ 0.00651731  0.00416489 -0.00975684 -0.00914209  0.00236854 -0.0022749\n",
      " -0.00141313 -0.00165362  0.0050057  -0.00563202  0.0032304   0.00618924\n",
      " -0.00167223 -0.00336334 -0.00120294  0.00335114 -0.00799667  0.00052977\n",
      " -0.00219113 -0.00862421  0.00421991 -0.00885975  0.00800045 -0.00403346\n",
      " -0.00253577  0.00787144 -0.00681122 -0.00163501 -0.00556823 -0.00661795\n",
      " -0.00242574  0.00104998  0.00762293 -0.00262424 -0.00929916  0.00877375\n",
      "  0.00150544 -0.00404584 -0.00722951 -0.00741862  0.00016465 -0.00139935\n",
      "  0.0073965  -0.00897764 -0.00221365 -0.00757297  0.00607856 -0.00621159\n",
      "  0.00248953 -0.00085829  0.00012888  0.00525149  0.0029597  -0.00796555\n",
      " -0.00097633 -0.00061736 -0.00211368 -0.00055841  0.008509    0.00785096\n",
      " -0.00314486 -0.00943737  0.00577965 -0.00058884 -0.00164371 -0.00113063\n",
      "  0.00927118 -0.00412405  0.00442258  0.00605141  0.00040647  0.00682351\n",
      "  0.00792841 -0.00751524 -0.00251856 -0.0003404  -0.00454092 -0.0067908\n",
      " -0.00319128 -0.00901452  0.00994171 -0.00293563  0.00367183 -0.0068227\n",
      " -0.00721638  0.00939729  0.00131727 -0.00612968 -0.00808855  0.00180283\n",
      "  0.0014842  -0.00727184 -0.00734775  0.00016885 -0.00494481  0.00980184\n",
      " -0.00679684 -0.00725201  0.0014393  -0.0045569 ]\n"
     ]
    }
   ],
   "source": [
    "# Example: Get word vectors for all tokens in the first row\n",
    "first_row_vectors = [model.wv[token] for token in df['tokens'][10] if token in model.wv]\n",
    "\n",
    "# Display word vectors\n",
    "print(\"Word Vectors for the first row tokens:\")\n",
    "for token, vector in zip(df['tokens'][500], first_row_vectors):\n",
    "    print(f\"Token: {token}, Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words for context: [('for', 0.9838205575942993), ('autism', 0.9808556437492371), ('autistic', 0.9801835417747498), ('are', 0.9790120720863342), ('or', 0.9789362549781799)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "# model = Word2Vec.load(\"your_model_path\")  # Uncomment to load an existing model\n",
    "\n",
    "# Function to predict a target word using CBOW\n",
    "def predict_target_word(context_words, model):\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    context_vectors = []\n",
    "    for word in context_words:\n",
    "        if word in model.wv:\n",
    "            context_vectors.append(model.wv[word])\n",
    "        else:\n",
    "            print(f\"Word '{word}' not in vocabulary.\")\n",
    "\n",
    "    # Check if we have valid context vectors\n",
    "    if not context_vectors:\n",
    "        return None\n",
    "    \n",
    "    # Average the context word vectors\n",
    "    context_vector = np.mean(context_vectors, axis=0)\n",
    "\n",
    "    # Find the most similar word to the context vector\n",
    "    similar_words = model.wv.similar_by_vector(context_vector, topn=5)  # Get top 5 similar words\n",
    "\n",
    "    return similar_words\n",
    "\n",
    "# Example sentence and context related to chloroplasts\n",
    "target_phrase = \"chloroplasts are vital for photosynthesis\"\n",
    "context_words = [\"chloroplasts\", \"are\", \"vital\", \"for\", \"photosynthesis\"]\n",
    "\n",
    "# Predicting the target word\n",
    "predicted_words = predict_target_word(context_words, model)\n",
    "\n",
    "# Display predicted words\n",
    "if predicted_words:\n",
    "    print(\"Predicted words for context:\", predicted_words)\n",
    "else:\n",
    "    print(\"No valid predictions could be made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a mapping from tokens to their indices\n",
    "token_to_index = {token: idx for idx, token in enumerate(set(tokens))}\n",
    "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "# Convert tokens to their corresponding indices\n",
    "numerical_data = [token_to_index[token] for token in tokens]\n",
    "\n",
    "# Prepare training samples (you can use a sliding window approach or any method suitable for your task)\n",
    "window_size = 2  # For context size\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(window_size, len(numerical_data) - window_size):\n",
    "    context = numerical_data[i - window_size:i] + numerical_data[i + 1:i + window_size + 1]\n",
    "    target = numerical_data[i]\n",
    "    X.append(context)\n",
    "    y.append(target)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Word2VecDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Get embeddings for context words\n",
    "        embeds = self.embeddings(context)\n",
    "        # Average the embeddings\n",
    "        avg_embed = embeds.mean(dim=1)\n",
    "        return self.linear(avg_embed)\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(token_to_index)\n",
    "embedding_dim = 100  # Same as Word2Vec vector size\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 1120/531413 [02:23<18:50:14,  7.82batch/s, loss=12]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[169], line 14\u001b[0m, in \u001b[0;36mWord2VecModel.forward\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Average the embeddings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m avg_embed \u001b[38;5;241m=\u001b[39m embeds\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_embed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Wrap your DataLoader with tqdm to show progress\n",
    "    with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for context, target in dataloader:\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            output = model(context)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            pbar.update(1)  # Update the progress bar\n",
    "            pbar.set_postfix(loss=loss.item())  # Optionally display loss value\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
