{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toeknising the titles \n",
    "We will now decapatilise and tokenise(segment) the titles so that we can process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing df of story rows rather that comments \n",
    "df = pd.read_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0          \"What May Happen in the Next Hundred Years\", f...\n",
       "1               Getting Started with JavaScript Unit Testing\n",
       "2          Armstrong, the Django-based and open-source ne...\n",
       "3                       Why Web Reviewers Make Up Bad Things\n",
       "4          You Weren't Meant to Have a Boss: The Cliff Notes\n",
       "                                 ...                        \n",
       "5351743    Scrapling: Fast, Adaptive Web Scraping for Python\n",
       "5351744                         Monkeys Predict US Elections\n",
       "5351745                                                  NaN\n",
       "5351746                                                  NaN\n",
       "5351747                                                  NaN\n",
       "Name: title, Length: 5351748, dtype: object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \n",
      "0     \"what may happen in the next hundred years\", f...         19.0  \n",
      "1          getting started with javascript unit testing          0.0  \n",
      "2     armstrong, the django-based and open-source ne...          0.0  \n",
      "3                  why web reviewers make up bad things          0.0  \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0  \n",
      "...                                                 ...          ...  \n",
      "2638  vmware communities roundtable podcast - show n...          0.0  \n",
      "2639                                     teeth problems         -1.0  \n",
      "2640  microsoft and nbc call it quits, for real: her...         -1.0  \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0  \n",
      "2642           microsoft hits back as google muscles in          1.0  \n",
      "\n",
      "[2123 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Set seed for consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Function to check if the title is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False  # If detection fails, we assume it's not English\n",
    "\n",
    "# Filter rows where the title is in English\n",
    "df['title'] = df['title'].str.lower()\n",
    "df = df[df['title'].apply(is_english)]\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\omare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \\\n",
      "0     \"what may happen in the next hundred years\", f...         19.0   \n",
      "1          getting started with javascript unit testing          0.0   \n",
      "2     armstrong, the django-based and open-source ne...          0.0   \n",
      "3                  why web reviewers make up bad things          0.0   \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0   \n",
      "...                                                 ...          ...   \n",
      "2638  vmware communities roundtable podcast - show n...          0.0   \n",
      "2639                                     teeth problems         -1.0   \n",
      "2640  microsoft and nbc call it quits, for real: her...         -1.0   \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0   \n",
      "2642           microsoft hits back as google muscles in          1.0   \n",
      "\n",
      "                                                stemmed  \\\n",
      "0     `` what may happen in the next hundr year '' ,...   \n",
      "1                   get start with javascript unit test   \n",
      "2     armstrong , the django-bas and open-sourc news...   \n",
      "3                      whi web review make up bad thing   \n",
      "4     you were n't meant to have a boss : the cliff ...   \n",
      "...                                                 ...   \n",
      "2638  vmware commun roundtabl podcast - show note # ...   \n",
      "2639                                      teeth problem   \n",
      "2640  microsoft and nbc call it quit , for real : he...   \n",
      "2641  woodstock insul side | jame hardi side | fiber...   \n",
      "2642               microsoft hit back as googl muscl in   \n",
      "\n",
      "                                             lemmatized  \n",
      "0     `` what may happen in the next hundred year ''...  \n",
      "1          getting started with javascript unit testing  \n",
      "2     armstrong , the django-based and open-source n...  \n",
      "3                    why web reviewer make up bad thing  \n",
      "4     you were n't meant to have a bos : the cliff note  \n",
      "...                                                 ...  \n",
      "2638  vmware community roundtable podcast - show not...  \n",
      "2639                                      teeth problem  \n",
      "2640  microsoft and nbc call it quits , for real : h...  \n",
      "2641  woodstock insulated siding | james hardie sidi...  \n",
      "2642              microsoft hit back a google muscle in  \n",
      "\n",
      "[2123 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import mapply\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply stemming and lemmatization\n",
    "def process_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(stemmed), ' '.join(lemmatized)\n",
    "\n",
    "# Apply the function to the 'title' column\n",
    "df['stemmed'], df['lemmatized'] = zip(*df['title'].apply(process_text))\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \\\n",
      "0     \"what may happen in the next hundred years\"<CO...         19.0   \n",
      "1          getting started with javascript unit testing          0.0   \n",
      "2     armstrong<COMMA> the django-based and open-sou...          0.0   \n",
      "3                  why web reviewers make up bad things          0.0   \n",
      "4     you weren't meant to have a boss: the cliff notes          1.0   \n",
      "...                                                 ...          ...   \n",
      "2638  vmware communities roundtable podcast - show n...          0.0   \n",
      "2639                                     teeth problems         -1.0   \n",
      "2640  microsoft and nbc call it quits<COMMA> for rea...         -1.0   \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0   \n",
      "2642           microsoft hits back as google muscles in          1.0   \n",
      "\n",
      "                                                stemmed  \\\n",
      "0     `` what may happen in the next hundr year '' ,...   \n",
      "1                   get start with javascript unit test   \n",
      "2     armstrong , the django-bas and open-sourc news...   \n",
      "3                      whi web review make up bad thing   \n",
      "4     you were n't meant to have a boss : the cliff ...   \n",
      "...                                                 ...   \n",
      "2638  vmware commun roundtabl podcast - show note # ...   \n",
      "2639                                      teeth problem   \n",
      "2640  microsoft and nbc call it quit , for real : he...   \n",
      "2641  woodstock insul side | jame hardi side | fiber...   \n",
      "2642               microsoft hit back as googl muscl in   \n",
      "\n",
      "                                             lemmatized  \\\n",
      "0     `` what may happen in the next hundred year ''...   \n",
      "1          getting started with javascript unit testing   \n",
      "2     armstrong , the django-based and open-source n...   \n",
      "3                    why web reviewer make up bad thing   \n",
      "4     you were n't meant to have a bos : the cliff note   \n",
      "...                                                 ...   \n",
      "2638  vmware community roundtable podcast - show not...   \n",
      "2639                                      teeth problem   \n",
      "2640  microsoft and nbc call it quits , for real : h...   \n",
      "2641  woodstock insulated siding | james hardie sidi...   \n",
      "2642              microsoft hit back a google muscle in   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [\"what, may, happen, in, the, next, hundred, y...   \n",
      "1     [getting, started, with, javascript, unit, tes...   \n",
      "2     [armstrong<COMMA>, the, django-based, and, ope...   \n",
      "3          [why, web, reviewers, make, up, bad, things]   \n",
      "4     [you, weren't, meant, to, have, a, boss:, the,...   \n",
      "...                                                 ...   \n",
      "2638  [vmware, communities, roundtable, podcast, -, ...   \n",
      "2639                                  [teeth, problems]   \n",
      "2640  [microsoft, and, nbc, call, it, quits<COMMA>, ...   \n",
      "2641  [woodstock, insulated, siding, |, james, hardi...   \n",
      "2642   [microsoft, hits, back, as, google, muscles, in]   \n",
      "\n",
      "                                              token_ids  \n",
      "0     [45, 3573, 2668, 2929, 5549, 3868, 2839, 6223,...  \n",
      "1                  [2502, 5277, 6130, 3150, 5813, 5537]  \n",
      "2     [705, 5549, 1820, 620, 4003, 3857, 1312, 3106,...  \n",
      "3             [6085, 6034, 4731, 3509, 5833, 809, 5572]  \n",
      "4     [6232, 6062, 3591, 5626, 2697, 429, 1002, 5549...  \n",
      "...                                                 ...  \n",
      "2638  [5956, 1378, 4800, 4290, 203, 5063, 3923, 51, ...  \n",
      "2639                                       [5513, 4402]  \n",
      "2640  [3652, 620, 3828, 1112, 3114, 4520, 2342, 4585...  \n",
      "2641  [6142, 3015, 5075, 6266, 3137, 2679, 5075, 626...  \n",
      "2642           [3652, 2769, 801, 717, 2554, 3780, 2929]  \n",
      "\n",
      "[2123 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to replace punctuation with special tokens\n",
    "def preprocess_text(text):\n",
    "    # Replace specific punctuation with special tokens\n",
    "    text = text.replace(',', '<COMMA>')  # Replace commas\n",
    "    text = text.replace('.', '<FULLSTOP>')  # Replace full stops\n",
    "    text = text.replace('!', '<EXCLAMATION>')  # Replace exclamations\n",
    "    return text\n",
    "\n",
    "# Tokenize the text (split into individual words and special tokens)\n",
    "def tokenize_text(text):\n",
    "    # Tokenize by splitting on whitespace and keeping punctuation tokens\n",
    "    return re.findall(r'\\S+|<COMMA>|<FULLSTOP>|<EXCLAMATION>', text)\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['tokens'] = df['title'].apply(tokenize_text)\n",
    "\n",
    "# Create a vocabulary (a dictionary mapping tokens to unique IDs)\n",
    "def create_vocabulary(tokens_series):\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "    # Create a unique token list and assign an ID to each\n",
    "    vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "    return vocabulary\n",
    "\n",
    "# Get vocabulary from the tokens in the DataFrame\n",
    "vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# Function to convert tokens to token IDs using the vocabulary\n",
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "# Apply the function to convert tokens to token IDs\n",
    "df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# Display the DataFrame with token IDs\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id dead   type              by                 time  \\\n",
      "0     3150000  NaN  story     ColinWright  2011-10-24 16:27:00   \n",
      "1     3500001  NaN  story            hncj  2012-01-23 11:39:25   \n",
      "2     3150001  NaN  story      andymboyle  2011-10-24 16:27:36   \n",
      "3     6050000  NaN  story         digisth  2013-07-16 05:16:26   \n",
      "4      150000    t  story         jazzdev  2008-03-30 09:46:25   \n",
      "...       ...  ...    ...             ...                  ...   \n",
      "2638  4250098  NaN  story        aluciani  2012-07-16 11:46:03   \n",
      "2639  4250101    t  story  davidhodges123  2012-07-16 11:46:34   \n",
      "2640  4250105    t  story   SlipperySlope  2012-07-16 11:49:03   \n",
      "2641  4250107    t  story    homewindow12  2012-07-16 11:49:38   \n",
      "2642  4250109  NaN  story   SlipperySlope  2012-07-16 11:50:02   \n",
      "\n",
      "                                                   text  parent  \\\n",
      "0                                                   NaN     NaN   \n",
      "1                                                   NaN     NaN   \n",
      "2                                                   NaN     NaN   \n",
      "3                                                   NaN     NaN   \n",
      "4                                                   NaN     NaN   \n",
      "...                                                 ...     ...   \n",
      "2638                                                NaN     NaN   \n",
      "2639  This article is really amazing as it provides ...     NaN   \n",
      "2640                                                NaN     NaN   \n",
      "2641  Home Improvement Experts in Chicago Illinois a...     NaN   \n",
      "2642                                                NaN     NaN   \n",
      "\n",
      "                                                   kids  \\\n",
      "0     {3150291,3150510,3150395,3150340,3150190,31526...   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                       {150001,150104}   \n",
      "...                                                 ...   \n",
      "2638                                                NaN   \n",
      "2639                                                NaN   \n",
      "2640                                                NaN   \n",
      "2641                                                NaN   \n",
      "2642                                          {4250645}   \n",
      "\n",
      "                                                    url  score  \\\n",
      "0     http://www.howtobearetronaut.com/wp-content/up...   19.0   \n",
      "1     http://blogs.lessthandot.com/index.php/WebDev/...    1.0   \n",
      "2     http://www.marketwatch.com/story/the-bay-citiz...    2.0   \n",
      "3     http://bits.blogs.nytimes.com/2013/07/15/why-w...    1.0   \n",
      "4                  http://paulgraham.com/bossnotes.html    1.0   \n",
      "...                                                 ...    ...   \n",
      "2638  http://blogs.vmware.com/vmtn/2012/07/vmware-co...    1.0   \n",
      "2639  http://ezinearticles.com/?Teeth-Cleaning-and-H...    1.0   \n",
      "2640  http://allthingsd.com/20120715/microsoft-and-n...    1.0   \n",
      "2641              http://www.innovativehomeconcepts.com    1.0   \n",
      "2642  http://online.wsj.com/article/SB10001424052702...    2.0   \n",
      "\n",
      "                                                  title  descendants  \\\n",
      "0     <DOUBLEQUOTE>what may happen in the next hundr...         19.0   \n",
      "1          getting started with javascript unit testing          0.0   \n",
      "2     armstrong<COMMA> the django<DASH>based and ope...          0.0   \n",
      "3                  why web reviewers make up bad things          0.0   \n",
      "4     you weren<QUOTE>t meant to have a boss<COLON> ...          1.0   \n",
      "...                                                 ...          ...   \n",
      "2638  vmware communities roundtable podcast <DASH> s...          0.0   \n",
      "2639                                     teeth problems         -1.0   \n",
      "2640  microsoft and nbc call it quits<COMMA> for rea...         -1.0   \n",
      "2641  woodstock insulated siding | james hardie sidi...         -1.0   \n",
      "2642           microsoft hits back as google muscles in          1.0   \n",
      "\n",
      "                                                stemmed  \\\n",
      "0     `` what may happen in the next hundr year '' ,...   \n",
      "1                   get start with javascript unit test   \n",
      "2     armstrong , the django-bas and open-sourc news...   \n",
      "3                      whi web review make up bad thing   \n",
      "4     you were n't meant to have a boss : the cliff ...   \n",
      "...                                                 ...   \n",
      "2638  vmware commun roundtabl podcast - show note # ...   \n",
      "2639                                      teeth problem   \n",
      "2640  microsoft and nbc call it quit , for real : he...   \n",
      "2641  woodstock insul side | jame hardi side | fiber...   \n",
      "2642               microsoft hit back as googl muscl in   \n",
      "\n",
      "                                             lemmatized  \\\n",
      "0     `` what may happen in the next hundred year ''...   \n",
      "1          getting started with javascript unit testing   \n",
      "2     armstrong , the django-based and open-source n...   \n",
      "3                    why web reviewer make up bad thing   \n",
      "4     you were n't meant to have a bos : the cliff note   \n",
      "...                                                 ...   \n",
      "2638  vmware community roundtable podcast - show not...   \n",
      "2639                                      teeth problem   \n",
      "2640  microsoft and nbc call it quits , for real : h...   \n",
      "2641  woodstock insulated siding | james hardie sidi...   \n",
      "2642              microsoft hit back a google muscle in   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [<DOUBLEQUOTE>what, may, happen, in, the, next...   \n",
      "1     [getting, started, with, javascript, unit, tes...   \n",
      "2     [armstrong<COMMA>, the, django<DASH>based, and...   \n",
      "3          [why, web, reviewers, make, up, bad, things]   \n",
      "4     [you, weren<QUOTE>t, meant, to, have, a, boss<...   \n",
      "...                                                 ...   \n",
      "2638  [vmware, communities, roundtable, podcast, <DA...   \n",
      "2639                                  [teeth, problems]   \n",
      "2640  [microsoft, and, nbc, call, it, quits<COMMA>, ...   \n",
      "2641  [woodstock, insulated, siding, |, james, hardi...   \n",
      "2642   [microsoft, hits, back, as, google, muscles, in]   \n",
      "\n",
      "                                              token_ids  \n",
      "0     [302, 3575, 2670, 2931, 5551, 3870, 2841, 6225...  \n",
      "1                  [2504, 5279, 6132, 3152, 5815, 5539]  \n",
      "2     [708, 5551, 1822, 623, 4005, 3859, 1314, 3108,...  \n",
      "3             [6087, 6036, 4733, 3511, 5835, 811, 5574]  \n",
      "4     [6234, 6064, 3593, 5628, 2699, 431, 1004, 5551...  \n",
      "...                                                 ...  \n",
      "2638  [5958, 1380, 4802, 4292, 256, 5065, 3925, 3, 2...  \n",
      "2639                                       [5515, 4405]  \n",
      "2640  [3654, 623, 3830, 1114, 3116, 4522, 2344, 4586...  \n",
      "2641  [6144, 3017, 5077, 6267, 3139, 2681, 5077, 626...  \n",
      "2642           [3654, 2771, 803, 720, 2556, 3782, 2931]  \n",
      "\n",
      "[2123 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to replace all punctuation with special tokens\n",
    "def preprocess_text(text):\n",
    "    # Replace various punctuation with special tokens\n",
    "    replacements = {\n",
    "        ',': '<COMMA>',\n",
    "        '.': '<FULLSTOP>',\n",
    "        '!': '<EXCLAMATION>',\n",
    "        '?': '<QUESTION>',\n",
    "        ';': '<SEMICOLON>',\n",
    "        ':': '<COLON>',\n",
    "        '(': '<OPENPAREN>',\n",
    "        ')': '<CLOSEPAREN>',\n",
    "        '[': '<OPENBRACKET>',\n",
    "        ']': '<CLOSEBRACKET>',\n",
    "        '{': '<OPENBRACE>',\n",
    "        '}': '<CLOSEBRACE>',\n",
    "        '\\'': '<QUOTE>',\n",
    "        '\\\"': '<DOUBLEQUOTE>',\n",
    "        '-': '<DASH>',\n",
    "        '_': '<UNDERLINE>',\n",
    "        '…': '<ELLIPSIS>'  # Ellipsis\n",
    "    }\n",
    "    \n",
    "    for punctuation, token in replacements.items():\n",
    "        text = text.replace(punctuation, token)  # Replace each punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize the text (split into individual words and special tokens)\n",
    "def tokenize_text(text):\n",
    "    # Tokenize by splitting on whitespace and keeping punctuation tokens\n",
    "    return re.findall(r'\\S+|<COMMA>|<FULLSTOP>|<EXCLAMATION>|<QUESTION>|<SEMICOLON>|<COLON>|<OPENPAREN>|<CLOSEPAREN>|<OPENBRACKET>|<CLOSEBRACKET>|<OPENBRACE>|<CLOSEBRACE>|<QUOTE>|<DOUBLEQUOTE>|<DASH>|<UNDERLINE>|<ELLIPSIS>', text)\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['tokens'] = df['title'].apply(tokenize_text)\n",
    "\n",
    "# Create a vocabulary (a dictionary mapping tokens to unique IDs)\n",
    "def create_vocabulary(tokens_series):\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "    # Create a unique token list and assign an ID to each\n",
    "    vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "    return vocabulary\n",
    "\n",
    "# Get vocabulary from the tokens in the DataFrame\n",
    "vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# Function to convert tokens to token IDs using the vocabulary\n",
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "# Apply the function to convert tokens to token IDs\n",
    "df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# Display the DataFrame with token IDs\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pandas as pd\n",
    "#import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Step 1: Preprocess the text (optional)\n",
    "# def preprocess_text(text):\n",
    "#     # Replace specific punctuation with special tokens\n",
    "#     text = text.replace(',', '<COMMA>')  # Replace commas\n",
    "#     text = text.replace('.', '<FULLSTOP>')  # Replace full stops\n",
    "#     text = text.replace('!', '<EXCLAMATION>')  # Replace exclamations\n",
    "#     return text\n",
    "\n",
    "# # Preprocess the 'title' column\n",
    "# df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# # Step 2: Save titles to a file for training SentencePiece\n",
    "# with open('titles.txt', 'w') as f:\n",
    "#     for title in df['title']:\n",
    "#         f.write(title + '\\n')\n",
    "\n",
    "# # Step 3: Train a SentencePiece model\n",
    "# # Increase vocab_size to at least 100 to avoid the error\n",
    "# spm.SentencePieceTrainer.train('--input=titles.txt --model_prefix=m --vocab_size=100 --character_coverage=1.0')\n",
    "\n",
    "# # Step 4: Load the trained SentencePiece model\n",
    "# sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "# # Tokenize the 'title' column using SentencePiece\n",
    "# df['tokens'] = df['title'].apply(lambda x: sp.encode(x, out_type=str))\n",
    "\n",
    "# # Create a vocabulary from the tokens\n",
    "# def create_vocabulary(tokens_series):\n",
    "#     all_tokens = [token for tokens in tokens_series for token in tokens]\n",
    "#     vocabulary = {token: idx + 1 for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "#     return vocabulary\n",
    "\n",
    "# # Get vocabulary from the tokens in the DataFrame\n",
    "# vocab = create_vocabulary(df['tokens'])\n",
    "\n",
    "# # Function to convert tokens to token IDs using the vocabulary\n",
    "# def tokens_to_ids(tokens, vocab):\n",
    "#     return [vocab[token] for token in tokens]\n",
    "\n",
    "# # Apply the function to convert tokens to token IDs\n",
    "# df['token_ids'] = df['tokens'].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n",
    "\n",
    "# # Display the DataFrame with tokens and token IDs\n",
    "# print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
