{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a5f534",
   "metadata": {},
   "source": [
    "# CBOW dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a911fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21555912",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278464a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the dataset: 500000\n"
     ]
    }
   ],
   "source": [
    "# Count total tokens in the entire dataset\n",
    "total_tokens = sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84c1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, max_tokens=20000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Limit to the first 2000 tokens\n",
    "    words = words[:max_tokens]\n",
    "\n",
    "    # Convert list of words into sequences of length `context_size`\n",
    "    sequence_length = 20  # Sequence length could be smaller for CBOW\n",
    "    sequences = [words[i:i + sequence_length] for i in range(0, len(words), sequence_length)]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Load the data\n",
    "sentences = load_and_preprocess_data('text8', max_tokens=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf26b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, min_count=5, context_size=2):\n",
    "        self.sentences = sentences\n",
    "        self.context_size = context_size\n",
    "        self.vocab = self.build_vocab(sentences, min_count)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                context = (\n",
    "                    sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "                )\n",
    "                target = sentence[i]\n",
    "                \n",
    "                if all(word in self.word_to_idx for word in context + [target]):\n",
    "                    self.data.append((context, target))\n",
    "        \n",
    "        # Debugging statement to check samples\n",
    "        print(f\"Generated {len(self.data)} samples from {len(sentences)} sentences\")\n",
    "\n",
    "    def build_vocab(self, sentences, min_count):\n",
    "        word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_idx[w] for w in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cdfe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Inputs are context word indices\n",
    "        embeds = self.embeddings(inputs)\n",
    "        context_embed = torch.mean(embeds, dim=1)  # Mean of context embeddings\n",
    "        output = self.linear(context_embed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b044a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, embedding_dim=512, batch_size=32, num_epochs=25, learning_rate=0.005):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context_words, target_word) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context_words, target_word = context_words.to(device), target_word.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context_words)\n",
    "            loss = criterion(log_probs, target_word)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "315293ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 117866 samples from 25000 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 662.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 7.3288, Val Loss: 6.8473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 626.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 6.5710, Val Loss: 6.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 614.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 6.3804, Val Loss: 6.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 609.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 6.2359, Val Loss: 6.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 596.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 6.1112, Val Loss: 6.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 603.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 5.9986, Val Loss: 6.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 603.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 5.8946, Val Loss: 6.5761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 618.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 5.7969, Val Loss: 6.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 597.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 5.7049, Val Loss: 6.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 629.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 5.6174, Val Loss: 6.5475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 606.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 5.5340, Val Loss: 6.5417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 611.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 5.4546, Val Loss: 6.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 594.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 5.3780, Val Loss: 6.5363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 594.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 5.3046, Val Loss: 6.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 599.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 5.2341, Val Loss: 6.5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 601.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 5.1659, Val Loss: 6.5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 600.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Train Loss: 5.1006, Val Loss: 6.5488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 597.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Train Loss: 5.0372, Val Loss: 6.5538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 603.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Train Loss: 4.9763, Val Loss: 6.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 605.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Train Loss: 4.9172, Val Loss: 6.5677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 605.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Train Loss: 4.8599, Val Loss: 6.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 596.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Train Loss: 4.8051, Val Loss: 6.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 599.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Train Loss: 4.7515, Val Loss: 6.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 593.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 4.6999, Val Loss: 6.6073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 593.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Train Loss: 4.6496, Val Loss: 6.6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:05<00:00, 588.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Train Loss: 4.6012, Val Loss: 6.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 596.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Train Loss: 4.5542, Val Loss: 6.6450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 605.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Train Loss: 4.5084, Val Loss: 6.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 613.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 4.4643, Val Loss: 6.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2947/2947 [00:04<00:00, 605.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 4.4211, Val Loss: 6.6831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_cbow_model(dataset, embedding_dim=50, batch_size=32, num_epochs=30, learning_rate=0.0005, val_split=0.2):\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model setup\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for context, target in val_dataloader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model with dataset split into training and validation sets\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=5)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b9bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 117866 samples from 25000 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 618.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 648.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 5.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 632.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 4.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 621.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:06<00:00, 610.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 2.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 616.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 2.2111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 618.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:06<00:00, 612.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 621.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:05<00:00, 625.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.1183\n"
     ]
    }
   ],
   "source": [
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=30, learning_rate=0.005):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=5)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "983d1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 117866 samples from 25000 sentences\n",
      "Context: ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'], Target: of\n",
      "Context: ['originated', 'as', 'a', 'term', 'of', 'first', 'used', 'against', 'early', 'working'], Target: abuse\n",
      "Context: ['as', 'a', 'term', 'of', 'abuse', 'used', 'against', 'early', 'working', 'class'], Target: first\n",
      "Context: ['a', 'term', 'of', 'abuse', 'first', 'against', 'early', 'working', 'class', 'radicals'], Target: used\n",
      "Context: ['term', 'of', 'abuse', 'first', 'used', 'early', 'working', 'class', 'radicals', 'including'], Target: against\n",
      "Context: ['of', 'abuse', 'first', 'used', 'against', 'working', 'class', 'radicals', 'including', 'the'], Target: early\n",
      "Context: ['of', 'the', 'french', 'revolution', 'whilst', 'term', 'is', 'still', 'used', 'in'], Target: the\n",
      "Context: ['the', 'french', 'revolution', 'whilst', 'the', 'is', 'still', 'used', 'in', 'a'], Target: term\n",
      "Context: ['describe', 'any', 'act', 'that', 'used', 'means', 'to', 'destroy', 'the', 'organization'], Target: violent\n"
     ]
    }
   ],
   "source": [
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=5)\n",
    "# Create an inverted dictionary for mapping indices back to words\n",
    "idx_to_word = {idx: word for word, idx in cbow_dataset.word_to_idx.items()}\n",
    "\n",
    "# Print out the context and target words for the first few samples\n",
    "for i in range(9):  # Change 5 to however many samples you'd like to see\n",
    "    context_indices, target_index = cbow_dataset[i]\n",
    "    context_words = [idx_to_word[idx.item()] for idx in context_indices]\n",
    "    target_word = idx_to_word[target_index.item()]\n",
    "    print(f\"Context: {context_words}, Target: {target_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea462ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWModel(\n",
       "  (embeddings): Embedding(8778, 128)\n",
       "  (linear): Linear(in_features=128, out_features=8778, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move model to the appropriate device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e46c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_sentences, dataset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for sentence in test_sentences:\n",
    "            # Get context words' indices\n",
    "            context_indices = [dataset.word_to_idx[word] for word in sentence]\n",
    "            context_tensor = torch.tensor(context_indices).unsqueeze(0).to(device)  # Move tensor to GPU\n",
    "\n",
    "            # Get the model's prediction\n",
    "            output = model(context_tensor)\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            # Convert the predicted index back to the word\n",
    "            predicted_word = dataset.idx_to_word[predicted_idx]\n",
    "            print(f\"Context: {sentence}, Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab0f9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as'], Predicted word: the\n",
      "Context: ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'], Predicted word: the\n",
      "Context: ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'], Predicted word: the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example sentences to evaluate the model\n",
    "test_sentences = [\n",
    "    ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as'],\n",
    "    ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'],\n",
    "    ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'],\n",
    "]\n",
    "\n",
    "# Evaluate the model using the test sentences\n",
    "evaluate_model(model, test_sentences, cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c05c9a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 120143 samples from 12500 sentences\n",
      "Vocabulary size: 5347\n",
      "Number of samples in the dataset: 120143\n"
     ]
    }
   ],
   "source": [
    "cbow_dataset = CBOWDataset(sentences)\n",
    "print(f\"Vocabulary size: {len(cbow_dataset.vocab)}\")\n",
    "print(f\"Number of samples in the dataset: {len(cbow_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2708abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_dataset = CBOWDataset(sentences, min_count=1)  # Lower min_count to include more words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7830dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
      "Sentence 2: ['revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to']\n",
      "Sentence 3: ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as']\n",
      "Sentence 4: ['a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king']\n",
      "Sentence 5: ['anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369f41-7f16-4454-9d1f-797e256c19ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
