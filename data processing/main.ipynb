{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdd1904-bfde-46ae-948d-b1e43a90d522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, sentences, window_size=10, min_count=3):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "        self.vocab = self._build_vocab(min_count)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.data = self._create_data()\n",
    "\n",
    "    def _build_vocab(self, min_count):\n",
    "        word_counts = Counter([word for sentence in self.sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def _create_data(self):\n",
    "        data = []\n",
    "        for sentence in self.sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word in self.word2idx:\n",
    "                    for j in range(max(0, i - self.window_size), min(len(sentence), i + self.window_size + 1)):\n",
    "                        if i != j and sentence[j] in self.word2idx:\n",
    "                            data.append((self.word2idx[word], self.word2idx[sentence[j]]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]).to(device), torch.tensor(self.data[idx][1]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ef154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, sentences, window_size=10, min_count=3):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "        self.vocab = self._build_vocab(min_count)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.data = self._create_data()\n",
    "\n",
    "    def _build_vocab(self, min_count):\n",
    "        word_counts = Counter([word for sentence in self.sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def _create_data(self):\n",
    "        data = []\n",
    "        for sentence in self.sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word in self.word2idx:\n",
    "                    for j in range(max(0, i - self.window_size), min(len(sentence), i + self.window_size + 1)):\n",
    "                        if i != j and sentence[j] in self.word2idx:\n",
    "                            data.append((self.word2idx[word], self.word2idx[sentence[j]]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]).to(device), torch.tensor(self.data[idx][1]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb71bfe-0ffb-4900-8b7f-8e31c581b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    print(f\"Total number of words: {len(words)}\")\n",
    "    \n",
    "    sequence_length = 20  \n",
    "    sequences = [words[i:i+sequence_length] for i in range(0, len(words), sequence_length)]\n",
    "    \n",
    "    print(f\"Number of sequences: {len(sequences)}\")\n",
    "    print(f\"First sequence: {sequences[0]}\")\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245f42d7-7806-4c2e-b2bd-1951fa2b5704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 17005207\n",
      "First 10 words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "Number of sequences: 850261\n",
      "First sequence: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "sentences=load_and_preprocess_data('data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef604454-f14e-44ae-9869-2a3ee8bbe42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[0:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68790ef2-eb4c-464c-8157-e0a77fbc2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42d2566-ebcc-4235-9016-bb6223a7c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296ea77c-7fe9-4510-a288-4a7b2563e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466068\n",
      "14565\n"
     ]
    }
   ],
   "source": [
    "print (len(dataset))\n",
    "print (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59521cd-43ef-4205-86a4-215238ce7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv')\n",
    "items = pd.read_csv('items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4eb432-8a6c-496a-88e0-d490217f35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output = self.linear(embeds)\n",
    "        return output\n",
    "        \n",
    "def train_model(dataset, embedding_dim=512, batch_size=32, num_epochs=25, learning_rate=0.005):\n",
    "    model = SkipGramModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (input_words, target_words) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            input_words, target_words = input_words.to(device), target_words.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(input_words)\n",
    "            loss = criterion(log_probs, target_words)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b8e7b0-d83c-4290-a85b-25d8293deecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:13<00:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 5.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14565/14565 [02:00<00:00, 120.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Loss: 5.6330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [04:35<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Loss: 5.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:11<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Loss: 5.5050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:10<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Loss: 5.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:37<00:00, 43.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Loss: 5.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:58<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Loss: 5.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:04<00:00, 39.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Loss: 5.3959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:25<00:00, 44.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Loss: 5.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:02<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Loss: 5.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:00<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Loss: 5.3526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:26<00:00, 44.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Loss: 5.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:50<00:00, 41.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Loss: 5.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:02<00:00, 40.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Loss: 5.3231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:04<00:00, 47.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Loss: 5.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:54<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Loss: 5.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:58<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Loss: 5.3021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14565/14565 [01:20<00:00, 180.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Loss: 5.2968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [04:18<00:00, 56.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Loss: 5.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:43<00:00, 42.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Loss: 5.2869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:49<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Loss: 5.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:47<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Loss: 5.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:55<00:00, 41.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Loss: 5.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:53<00:00, 41.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Loss: 5.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:41<00:00, 42.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Loss: 5.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f0e78d-c0d5-4885-881e-7f528c873c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "def most_similar(word, top_n=5):\n",
    "    if word not in dataset.word2idx:\n",
    "        return []\n",
    "    word_idx = dataset.word2idx[word]\n",
    "    word_vec = word_embeddings[word_idx]\n",
    "    similarities = np.dot(word_embeddings, word_vec) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vec))\n",
    "    most_similar = similarities.argsort()[-top_n-1:-1][::-1]\n",
    "    return [(dataset.idx2word[idx], similarities[idx]) for idx in most_similar if idx != word_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f95f419a-0a5c-4d4f-b892-09c56e3c026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('famous', 0.49498144), ('difference', 0.46292886), ('woman', 0.4597501), ('influential', 0.45435798), ('kingdom', 0.4328415)]\n"
     ]
    }
   ],
   "source": [
    "print(most_similar(\"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2daa26d-37f6-4b9b-95d5-c7f6c3b7229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, 'checkpoints/initial.pkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65a9dbfb-6504-4ce8-affc-c0d8a49045e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/initial.pkg', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a86ffc94-c75a-495b-9215-2e8be9786a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embeddings.weight',\n",
       "              tensor([[-0.0334,  0.7723, -0.1541,  ..., -0.8312, -0.8679, -0.1490],\n",
       "                      [ 0.1979,  0.0738, -0.0707,  ..., -0.6376,  0.0895, -0.0600],\n",
       "                      [-0.1665, -0.0984,  0.0231,  ...,  0.0484,  0.0600, -0.0972],\n",
       "                      ...,\n",
       "                      [-0.0420,  1.2516,  0.3799,  ...,  1.1112,  0.0416,  1.6234],\n",
       "                      [-0.4737,  0.3162,  0.9902,  ...,  0.9877, -0.2667,  0.8550],\n",
       "                      [-0.9057,  0.2380,  0.2295,  ...,  0.0347, -0.9295, -0.0289]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.weight',\n",
       "              tensor([[-0.8265, -0.0731, -0.6963,  ..., -0.0869, -0.9737,  1.0510],\n",
       "                      [-0.7041, -0.2857, -0.5386,  ..., -0.0798, -0.6669,  1.2450],\n",
       "                      [-0.6976, -0.2072, -0.5487,  ..., -0.0528, -0.7097,  1.0454],\n",
       "                      ...,\n",
       "                      [-0.7761, -0.2400, -0.5212,  ...,  0.3490, -0.9173,  1.4619],\n",
       "                      [-0.4572, -0.1212,  0.2855,  ...,  0.2221, -0.6702,  1.4598],\n",
       "                      [-0.8360,  0.1635, -0.6972,  ...,  0.1148, -1.0991,  1.0652]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.9497,  0.3454,  0.8773,  ..., -1.1699, -1.1068, -1.0300],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5f534",
   "metadata": {},
   "source": [
    "# CBOW dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56a911fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21555912",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "278464a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the dataset: 20000\n"
     ]
    }
   ],
   "source": [
    "# Count total tokens in the entire dataset\n",
    "total_tokens = sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d84c1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, max_tokens=20000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Limit to the first 2000 tokens\n",
    "    words = words[:max_tokens]\n",
    "\n",
    "    # Convert list of words into sequences of length `context_size`\n",
    "    sequence_length = 20  # Sequence length could be smaller for CBOW\n",
    "    sequences = [words[i:i + sequence_length] for i in range(0, len(words), sequence_length)]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Load the data\n",
    "sentences = load_and_preprocess_data('text8.txt', max_tokens=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9bf26b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, min_count=5, context_size=2):\n",
    "        self.sentences = sentences\n",
    "        self.context_size = context_size\n",
    "        self.vocab = self.build_vocab(sentences, min_count)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                context = (\n",
    "                    sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "                )\n",
    "                target = sentence[i]\n",
    "                \n",
    "                if all(word in self.word_to_idx for word in context + [target]):\n",
    "                    self.data.append((context, target))\n",
    "        \n",
    "        # Debugging statement to check samples\n",
    "        print(f\"Generated {len(self.data)} samples from {len(sentences)} sentences\")\n",
    "\n",
    "    def build_vocab(self, sentences, min_count):\n",
    "        word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_idx[w] for w in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6cdfe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Inputs are context word indices\n",
    "        embeds = self.embeddings(inputs)\n",
    "        context_embed = torch.mean(embeds, dim=1)  # Mean of context embeddings\n",
    "        output = self.linear(context_embed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b044a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, embedding_dim=512, batch_size=32, num_epochs=25, learning_rate=0.005):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context_words, target_word) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context_words, target_word = context_words.to(device), target_word.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context_words)\n",
    "            loss = criterion(log_probs, target_word)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45b9bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 117866 samples from 25000 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 202/3684 [00:08<02:27, 23.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m cbow_dataset \u001b[38;5;241m=\u001b[39m CBOWDataset(sentences, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cbow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbow_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[83], line 15\u001b[0m, in \u001b[0;36mtrain_cbow_model\u001b[1;34m(dataset, embedding_dim, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     13\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(context)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(log_probs, target)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=10, learning_rate=0.005):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=5)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "983d1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['anarchism', 'originated', 'a', 'term'], Target: as\n",
      "Context: ['originated', 'as', 'term', 'of'], Target: a\n",
      "Context: ['as', 'a', 'of', 'abuse'], Target: term\n",
      "Context: ['a', 'term', 'abuse', 'first'], Target: of\n",
      "Context: ['term', 'of', 'first', 'used'], Target: abuse\n"
     ]
    }
   ],
   "source": [
    "# Create an inverted dictionary for mapping indices back to words\n",
    "idx_to_word = {idx: word for word, idx in cbow_dataset.word_to_idx.items()}\n",
    "\n",
    "# Print out the context and target words for the first few samples\n",
    "for i in range(5):  # Change 5 to however many samples you'd like to see\n",
    "    context_indices, target_index = cbow_dataset[i]\n",
    "    context_words = [idx_to_word[idx.item()] for idx in context_indices]\n",
    "    target_word = idx_to_word[target_index.item()]\n",
    "    print(f\"Context: {context_words}, Target: {target_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab0f9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['anarchism', 'originated', 'a', 'term'], Target: as, Predicted: have\n",
      "Context: ['originated', 'as', 'term', 'of'], Target: a, Predicted: abuse\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, test_sentences, cbow_dataset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for sentence in test_sentences:\n",
    "            for i in range(2, len(sentence) - 2):\n",
    "                # Extract context and target\n",
    "                context = sentence[i - 2:i] + sentence[i + 1:i + 3]\n",
    "                target = sentence[i]\n",
    "                \n",
    "                # Convert context to indices\n",
    "                context_indices = [cbow_dataset.word_to_idx[word] for word in context]\n",
    "                \n",
    "                # Convert to tensor and reshape for model input\n",
    "                context_tensor = torch.tensor(context_indices).unsqueeze(0)  # Shape (1, context_size)\n",
    "                \n",
    "                # Get the model's prediction\n",
    "                output = model(context_tensor)\n",
    "                predicted_idx = torch.argmax(output, dim=1).item()\n",
    "                \n",
    "                # Convert the predicted index back to the word\n",
    "                predicted_word = cbow_dataset.idx_to_word[predicted_idx]\n",
    "                \n",
    "                # Compare prediction with target\n",
    "                if predicted_word == target:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                \n",
    "                print(f\"Context: {context}, Target: {target}, Predicted: {predicted_word}\")\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Example sentences to evaluate the model\n",
    "test_sentences = [\n",
    "    ['anarchism', 'originated', 'as', 'a', 'term'],\n",
    "    ['originated', 'as', 'a', 'term', 'of']\n",
    "]\n",
    "\n",
    "# Evaluate the model using the test sentences\n",
    "evaluate_model(model, test_sentences, cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c05c9a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 120143 samples from 12500 sentences\n",
      "Vocabulary size: 5347\n",
      "Number of samples in the dataset: 120143\n"
     ]
    }
   ],
   "source": [
    "cbow_dataset = CBOWDataset(sentences)\n",
    "print(f\"Vocabulary size: {len(cbow_dataset.vocab)}\")\n",
    "print(f\"Number of samples in the dataset: {len(cbow_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2708abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_dataset = CBOWDataset(sentences, min_count=1)  # Lower min_count to include more words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7830dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
      "Sentence 2: ['revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to']\n",
      "Sentence 3: ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as']\n",
      "Sentence 4: ['a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king']\n",
      "Sentence 5: ['anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef587a-754c-4d06-b46d-fbe42144f187",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74abc57-d2ee-4de4-98e4-0fee8d36c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract month from the 'time' column\n",
    "df['month'] = df['time'].dt.strftime('%B')\n",
    "\n",
    "# Count submissions for each month\n",
    "monthly_submissions = df['month'].value_counts().sort_index()\n",
    "\n",
    "# Define month order for proper sorting\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Reindex the series to ensure all months are included and properly ordered\n",
    "monthly_submissions = monthly_submissions.reindex(month_order).fillna(0)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_submissions.plot(kind='bar')\n",
    "plt.title('Number of Hacker News Submissions per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdace3-1f4a-4dd0-b829-d2f8fa000ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "\n",
    "# Count submissions for each year\n",
    "yearly_submissions = df['year'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_submissions.plot(kind='bar')\n",
    "plt.title('Number of Hacker News Submissions per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.xticks(rotation=0)  # Keeping year labels horizontal\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print the data\n",
    "print(yearly_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd13f7-0125-49a5-8b11-ecee22a33fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by year and month\n",
    "monthly_submissions = monthly_submissions.sort_values(['year', 'month'])\n",
    "\n",
    "# Create a sequential index for x-axis\n",
    "monthly_submissions['index'] = range(len(monthly_submissions))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get unique years for different colors\n",
    "years = monthly_submissions['year'].unique()\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(years))\n",
    "\n",
    "# Plot each year with a different color\n",
    "for year, color in zip(years, colors):\n",
    "    year_data = monthly_submissions[monthly_submissions['year'] == year]\n",
    "    plt.plot(year_data['index'], year_data['count'], label=str(year), color=color)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions per Month (Colored by Year)')\n",
    "plt.xlabel('Months (Sequential)')\n",
    "plt.ylabel('Number of Submissions')\n",
    "\n",
    "# Customize x-axis ticks to show every 12th month (January of each year)\n",
    "xticks = monthly_submissions[monthly_submissions['month'] == 1]['index']\n",
    "xtick_labels = monthly_submissions[monthly_submissions['month'] == 1]['year']\n",
    "plt.xticks(xticks, xtick_labels, rotation=45)\n",
    "\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ade83-c886-4f4c-b919-6bb8e445f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by year and month\n",
    "monthly_submissions = monthly_submissions.sort_values(['year', 'month'])\n",
    "\n",
    "# Create a sequential index for x-axis\n",
    "monthly_submissions['index'] = range(len(monthly_submissions))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Get unique years for different colors\n",
    "years = monthly_submissions['year'].unique()\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(years))\n",
    "\n",
    "# Plot bars for each month, colored by year\n",
    "for year, color in zip(years, colors):\n",
    "    year_data = monthly_submissions[monthly_submissions['year'] == year]\n",
    "    plt.bar(year_data['index'], year_data['count'], color=color, width=1, align='edge')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions per Month (Colored by Year)', fontsize=16)\n",
    "plt.xlabel('Months (Sequential)', fontsize=12)\n",
    "plt.ylabel('Number of Submissions', fontsize=12)\n",
    "\n",
    "# Customize x-axis ticks to show every 12th month (January of each year)\n",
    "xticks = monthly_submissions[monthly_submissions['month'] == 1]['index']\n",
    "xtick_labels = monthly_submissions[monthly_submissions['month'] == 1]['year']\n",
    "plt.xticks(xticks, xtick_labels, rotation=45, ha='right')\n",
    "\n",
    "# Add a legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors]\n",
    "plt.legend(handles, years, title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba779-5362-4540-bea3-4e262b5bb14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02274a43-7f72-4ae8-a376-8784434fecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the data to have years as columns and months as rows\n",
    "pivot_data = monthly_submissions.pivot(index='month', columns='year', values='count')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot lines for each year\n",
    "for year in pivot_data.columns:\n",
    "    plt.plot(pivot_data.index, pivot_data[year], label=str(year), marker='o', markersize=4)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions by Month and Year', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Submissions', fontsize=12)\n",
    "\n",
    "# Set x-axis ticks to show month names\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(range(1, 13), month_names)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369f41-7f16-4454-9d1f-797e256c19ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
