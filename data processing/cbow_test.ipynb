{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a5f534",
   "metadata": {},
   "source": [
    "# CBOW dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a911fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21555912",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278464a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Count total tokens in the entire dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msentences\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of tokens in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Count total tokens in the entire dataset\n",
    "total_tokens = sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84c1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, max_tokens=20000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Limit to the first 2000 tokens\n",
    "    words = words[:max_tokens]\n",
    "\n",
    "    # Convert list of words into sequences of length `context_size`\n",
    "    sequence_length = 20  # Sequence length could be smaller for CBOW\n",
    "    sequences = [words[i:i + sequence_length] for i in range(0, len(words), sequence_length)]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Load the data\n",
    "sentences = load_and_preprocess_data('text8', max_tokens=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf26b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, min_count=5, context_size=2):\n",
    "        self.sentences = sentences\n",
    "        self.context_size = context_size\n",
    "        self.vocab = self.build_vocab(sentences, min_count)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                context = (\n",
    "                    sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "                )\n",
    "                target = sentence[i]\n",
    "                \n",
    "                if all(word in self.word_to_idx for word in context + [target]):\n",
    "                    self.data.append((context, target))\n",
    "        \n",
    "        # Debugging statement to check samples\n",
    "        print(f\"Generated {len(self.data)} samples from {len(sentences)} sentences\")\n",
    "\n",
    "    def build_vocab(self, sentences, min_count):\n",
    "        word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_idx[w] for w in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdfe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CBOWModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim):\n",
    "#         super(CBOWModel, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # Inputs are context word indices\n",
    "#         embeds = self.embeddings(inputs)\n",
    "#         context_embed = torch.mean(embeds, dim=1)  # Mean of context embeddings\n",
    "#         output = self.linear(context_embed)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72019de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        # Get the embeddings for the context words\n",
    "        x = self.embeddings(inputs_)\n",
    "        # Mean of the context word embeddings\n",
    "        x = x.mean(axis=1)\n",
    "        # Pass through linear layer to predict target word\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b8ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, min_count=5, context_size=2):\n",
    "        self.sentences = sentences\n",
    "        self.context_size = context_size\n",
    "        self.vocab = self.build_vocab(sentences, min_count)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Ensure enough context around the target word\n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                context = (\n",
    "                    sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "                )\n",
    "                target = sentence[i]\n",
    "                \n",
    "                if all(word in self.word_to_idx for word in context + [target]):\n",
    "                    self.data.append((context, target))\n",
    "        \n",
    "        print(f\"Generated {len(self.data)} samples from {len(sentences)} sentences\")\n",
    "\n",
    "    def build_vocab(self, sentences, min_count):\n",
    "        word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_idx[w] for w in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97de6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=50, learning_rate=0.0001, val_split=0.2):\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    wandb.init(project=\"cbow_training\", entity=\"omareweis123\")  # Initialize W&B\n",
    "    \n",
    "    wandb.config.update({\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"val_split\": val_split\n",
    "    })\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch_idx, (context, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)  # Get raw logits from the model\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            wandb.log({\"batch_train_loss\": loss.item(), \"epoch\": epoch + 1})  # Log batch loss\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for context, target in val_dataloader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "        # Log epoch-level losses\n",
    "        wandb.log({\n",
    "            \"epoch_train_loss\": avg_train_loss,\n",
    "            \"epoch_val_loss\": avg_val_loss,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"cbow_model_weights.pth\")\n",
    "    wandb.finish()  # Complete the W&B run\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed70c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 276575 samples from 25000 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\hackernews_predicting_views\\data processing\\wandb\\run-20241016_153034-d7b0texr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/cbow_training/runs/d7b0texr' target=\"_blank\">silver-cloud-13</a></strong> to <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/cbow_training/runs/d7b0texr' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/d7b0texr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 626.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 6.3153, Val Loss: 5.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 581.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 4.9572, Val Loss: 5.9730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 571.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 4.2072, Val Loss: 6.1804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 3.7067, Val Loss: 6.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 576.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 3.3705, Val Loss: 6.7398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 583.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 3.1385, Val Loss: 7.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 576.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 2.9711, Val Loss: 7.2708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 563.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 2.8491, Val Loss: 7.4950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 570.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 2.7519, Val Loss: 7.7207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 2.6750, Val Loss: 7.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 576.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 2.6124, Val Loss: 8.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 565.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 2.5641, Val Loss: 8.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 2.5200, Val Loss: 8.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 2.4836, Val Loss: 8.6329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 582.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 2.4493, Val Loss: 8.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 577.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 2.4224, Val Loss: 8.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 571.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 2.3980, Val Loss: 9.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 574.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 2.3763, Val Loss: 9.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 574.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 2.3538, Val Loss: 9.3510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 569.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 2.3383, Val Loss: 9.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 2.3215, Val Loss: 9.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 566.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 2.3046, Val Loss: 9.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 581.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 2.2907, Val Loss: 9.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 577.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 2.2781, Val Loss: 9.9471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 570.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 2.2676, Val Loss: 10.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 565.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 2.2536, Val Loss: 10.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 2.2461, Val Loss: 10.2743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 2.2383, Val Loss: 10.3647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 2.2292, Val Loss: 10.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 2.2206, Val Loss: 10.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 569.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 2.2142, Val Loss: 10.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 2.2034, Val Loss: 10.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 2.1976, Val Loss: 10.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 567.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 2.1903, Val Loss: 10.9344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 568.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 2.1832, Val Loss: 11.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 564.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss: 2.1767, Val Loss: 11.1619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 572.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss: 2.1762, Val Loss: 11.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 570.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss: 2.1668, Val Loss: 11.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 569.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 2.1612, Val Loss: 11.4016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 564.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 2.1550, Val Loss: 11.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:11<00:00, 577.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 2.1503, Val Loss: 11.5787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 573.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 2.1490, Val Loss: 11.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 566.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 2.1447, Val Loss: 11.7084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 562.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 2.1373, Val Loss: 11.8070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 572.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss: 2.1343, Val Loss: 11.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 574.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 2.1320, Val Loss: 11.9717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 570.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 2.1281, Val Loss: 12.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 575.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 2.1207, Val Loss: 12.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 572.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 2.1199, Val Loss: 12.2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6915/6915 [00:12<00:00, 571.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 2.1153, Val Loss: 12.2943\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_train_loss</td><td>█▇▄▅▃▄▂▂▂▄▂▁▂▂▃▂▂▂▃▃▁▂▃▂▃▂▂▃▁▂▂▂▂▂▂▃▁▂▃▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_train_loss</td><td>█▆▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch_val_loss</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_train_loss</td><td>1.25188</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>epoch_train_loss</td><td>2.11531</td></tr><tr><td>epoch_val_loss</td><td>12.29427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-cloud-13</strong> at: <a href='https://wandb.ai/omareweis123/cbow_training/runs/d7b0texr' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/d7b0texr</a><br/> View project at: <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241016_153034-d7b0texr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model with dataset split into training and validation sets\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=2)\n",
    "model = train_cbow_model(cbow_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b044a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, embedding_dim=512, batch_size=32, num_epochs=25, learning_rate=0.005):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context_words, target_word) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context_words, target_word = context_words.to(device), target_word.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context_words)\n",
    "            loss = criterion(log_probs, target_word)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "315293ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 276575 samples from 25000 sentences\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CBOWModel.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model with dataset split into training and validation sets\u001b[39;00m\n\u001b[0;32m     50\u001b[0m cbow_dataset \u001b[38;5;241m=\u001b[39m CBOWDataset(sentences, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cbow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbow_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mtrain_cbow_model\u001b[1;34m(dataset, embedding_dim, batch_size, num_epochs, learning_rate, val_split)\u001b[0m\n\u001b[0;32m     10\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Model setup\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCBOWModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mTypeError\u001b[0m: CBOWModel.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=5, learning_rate=0.005, val_split=0.2):\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model setup\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for context, target in val_dataloader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model with dataset split into training and validation sets\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=2)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed84944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\hackernews_predicting_views\\data processing\\wandb\\run-20241016_150504-66ar58ww</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/cbow_training/runs/66ar58ww' target=\"_blank\">fluent-dream-11</a></strong> to <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/cbow_training' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/cbow_training/runs/66ar58ww' target=\"_blank\">https://wandb.ai/omareweis123/cbow_training/runs/66ar58ww</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 276575 samples from 25000 sentences\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Train the model with dataset split into training and validation sets\u001b[39;00m\n\u001b[0;32m     84\u001b[0m cbow_dataset \u001b[38;5;241m=\u001b[39m CBOWDataset(sentences, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cbow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbow_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain_cbow_model\u001b[1;34m(dataset, embedding_dim, batch_size, num_epochs, learning_rate, val_split)\u001b[0m\n\u001b[0;32m     17\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Model setup\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCBOWModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m, in \u001b[0;36mCBOWModel.__init__\u001b[1;34m(self, vocab_size, embedding_dim)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size: \u001b[38;5;28mint\u001b[39m,embedding_dim):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCBOW_Model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\n\u001b[0;32m     11\u001b[0m         num_embeddings\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[0;32m     12\u001b[0m         embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[0;32m     15\u001b[0m         in_features\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[0;32m     16\u001b[0m         out_features\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[0;32m     17\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"cbow_training\", entity=\"omareweis123\")  # Replace with your W&B username\n",
    "\n",
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=5, learning_rate=0.005, val_split=0.2):\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model setup\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Track hyperparameters and model architecture in W&B\n",
    "    wandb.config.update({\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"val_split\": val_split\n",
    "    })\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Log batch loss to W&B\n",
    "            wandb.log({\"batch_train_loss\": loss.item(), \"epoch\": epoch+1})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for context, target in val_dataloader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "        # Log epoch-level train/val losses to W&B\n",
    "        wandb.log({\n",
    "            \"epoch_train_loss\": avg_train_loss,\n",
    "            \"epoch_val_loss\": avg_val_loss,\n",
    "            \"epoch\": epoch+1\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the model weights\n",
    "    torch.save(model.state_dict(), \"cbow_model_weights.pth\")\n",
    "\n",
    "    WANDB_HTTP_TIMEOUT=500\n",
    "    \n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model with dataset split into training and validation sets\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=2)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b9bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3525551 samples from 250000 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:29<00:00, 526.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.5378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:31<00:00, 520.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 6.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:32<00:00, 518.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 6.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:32<00:00, 518.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 6.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:27<00:00, 530.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:29<00:00, 527.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 6.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:30<00:00, 522.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 6.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:25<00:00, 535.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 5.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:25<00:00, 534.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 5.9935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110174/110174 [03:32<00:00, 519.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 5.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_cbow_model(dataset, embedding_dim=128, batch_size=32, num_epochs=10, learning_rate=0.05):\n",
    "    model = CBOWModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (context, target) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = criterion(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=2)\n",
    "model = train_cbow_model(cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983d1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 117866 samples from 25000 sentences\n",
      "Context: ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early'], Target: of\n",
      "Context: ['originated', 'as', 'a', 'term', 'of', 'first', 'used', 'against', 'early', 'working'], Target: abuse\n",
      "Context: ['as', 'a', 'term', 'of', 'abuse', 'used', 'against', 'early', 'working', 'class'], Target: first\n",
      "Context: ['a', 'term', 'of', 'abuse', 'first', 'against', 'early', 'working', 'class', 'radicals'], Target: used\n",
      "Context: ['term', 'of', 'abuse', 'first', 'used', 'early', 'working', 'class', 'radicals', 'including'], Target: against\n",
      "Context: ['of', 'abuse', 'first', 'used', 'against', 'working', 'class', 'radicals', 'including', 'the'], Target: early\n",
      "Context: ['of', 'the', 'french', 'revolution', 'whilst', 'term', 'is', 'still', 'used', 'in'], Target: the\n",
      "Context: ['the', 'french', 'revolution', 'whilst', 'the', 'is', 'still', 'used', 'in', 'a'], Target: term\n",
      "Context: ['describe', 'any', 'act', 'that', 'used', 'means', 'to', 'destroy', 'the', 'organization'], Target: violent\n"
     ]
    }
   ],
   "source": [
    "cbow_dataset = CBOWDataset(sentences, min_count=5, context_size=5)\n",
    "# Create an inverted dictionary for mapping indices back to words\n",
    "idx_to_word = {idx: word for word, idx in cbow_dataset.word_to_idx.items()}\n",
    "\n",
    "# Print out the context and target words for the first few samples\n",
    "for i in range(9):  # Change 5 to however many samples you'd like to see\n",
    "    context_indices, target_index = cbow_dataset[i]\n",
    "    context_words = [idx_to_word[idx.item()] for idx in context_indices]\n",
    "    target_word = idx_to_word[target_index.item()]\n",
    "    print(f\"Context: {context_words}, Target: {target_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05c9a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1934891 samples from 250000 sentences\n",
      "Vocabulary size: 36280\n",
      "Number of samples in the dataset: 1934891\n"
     ]
    }
   ],
   "source": [
    "cbow_dataset = CBOWDataset(sentences,min_count=5, context_size=5)\n",
    "print(f\"Vocabulary size: {len(cbow_dataset.vocab)}\")\n",
    "print(f\"Number of samples in the dataset: {len(cbow_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea462ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWModel(\n",
       "  (embeddings): Embedding(36280, 128)\n",
       "  (linear): Linear(in_features=128, out_features=36280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move model to the appropriate device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e46c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_sentences, dataset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for sentence in test_sentences:\n",
    "            # Get context words' indices\n",
    "            context_indices = [dataset.word_to_idx[word] for word in sentence]\n",
    "            context_tensor = torch.tensor(context_indices).unsqueeze(0).to(device)  # Move tensor to GPU\n",
    "\n",
    "            # Get the model's prediction\n",
    "            output = model(context_tensor)\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            # Convert the predicted index back to the word\n",
    "            predicted_word = dataset.idx_to_word[predicted_idx]\n",
    "            print(f\"Context: {sentence}, Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0f9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as'], Predicted word: the\n",
      "Context: ['term', 'of', 'abuse', 'first', 'used', 'early', 'working', 'class', 'radicals', 'including'], Predicted word: the\n",
      "Context: ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against'], Predicted word: the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example sentences to evaluate the model\n",
    "test_sentences = [\n",
    "    ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as'],\n",
    "    ['term', 'of', 'abuse', 'first', 'used', 'early', 'working', 'class', 'radicals', 'including'],\n",
    "    ['anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against'],\n",
    "]\n",
    "\n",
    "# Evaluate the model using the test sentences\n",
    "evaluate_model(model, test_sentences, cbow_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2708abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_dataset = CBOWDataset(sentences, min_count=1)  # Lower min_count to include more words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7830dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
      "Sentence 2: ['revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to']\n",
      "Sentence 3: ['describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as']\n",
      "Sentence 4: ['a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king']\n",
      "Sentence 5: ['anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369f41-7f16-4454-9d1f-797e256c19ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
