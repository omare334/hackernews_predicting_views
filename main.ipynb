{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdd1904-bfde-46ae-948d-b1e43a90d522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, sentences, window_size=10, min_count=3):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "        self.vocab = self._build_vocab(min_count)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.data = self._create_data()\n",
    "\n",
    "    def _build_vocab(self, min_count):\n",
    "        word_counts = Counter([word for sentence in self.sentences for word in sentence])\n",
    "        return [word for word, count in word_counts.items() if count >= min_count]\n",
    "\n",
    "    def _create_data(self):\n",
    "        data = []\n",
    "        for sentence in self.sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word in self.word2idx:\n",
    "                    for j in range(max(0, i - self.window_size), min(len(sentence), i + self.window_size + 1)):\n",
    "                        if i != j and sentence[j] in self.word2idx:\n",
    "                            data.append((self.word2idx[word], self.word2idx[sentence[j]]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]).to(device), torch.tensor(self.data[idx][1]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb71bfe-0ffb-4900-8b7f-8e31c581b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    print(f\"Total number of words: {len(words)}\")\n",
    "    \n",
    "    sequence_length = 20  \n",
    "    sequences = [words[i:i+sequence_length] for i in range(0, len(words), sequence_length)]\n",
    "    \n",
    "    print(f\"Number of sequences: {len(sequences)}\")\n",
    "    print(f\"First sequence: {sequences[0]}\")\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245f42d7-7806-4c2e-b2bd-1951fa2b5704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 17005207\n",
      "First 10 words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "Number of sequences: 850261\n",
      "First sequence: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "sentences=load_and_preprocess_data('data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef604454-f14e-44ae-9869-2a3ee8bbe42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[0:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68790ef2-eb4c-464c-8157-e0a77fbc2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42d2566-ebcc-4235-9016-bb6223a7c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296ea77c-7fe9-4510-a288-4a7b2563e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466068\n",
      "14565\n"
     ]
    }
   ],
   "source": [
    "print (len(dataset))\n",
    "print (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70665634-b5a5-456b-bb55-016f54daaa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59521cd-43ef-4205-86a4-215238ce7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('data/users.csv')\n",
    "items = pd.read_csv('data/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4eb432-8a6c-496a-88e0-d490217f35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output = self.linear(embeds)\n",
    "        return output\n",
    "        \n",
    "def train_model(dataset, embedding_dim=512, batch_size=32, num_epochs=25, learning_rate=0.005):\n",
    "    model = SkipGramModel(len(dataset.vocab), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (input_words, target_words) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            input_words, target_words = input_words.to(device), target_words.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(input_words)\n",
    "            loss = criterion(log_probs, target_words)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b8e7b0-d83c-4290-a85b-25d8293deecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:13<00:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 5.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14565/14565 [02:00<00:00, 120.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Loss: 5.6330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [04:35<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Loss: 5.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:11<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Loss: 5.5050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:10<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Loss: 5.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:37<00:00, 43.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Loss: 5.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:58<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Loss: 5.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:04<00:00, 39.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Loss: 5.3959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:25<00:00, 44.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Loss: 5.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:02<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Loss: 5.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:00<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Loss: 5.3526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:26<00:00, 44.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Loss: 5.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:50<00:00, 41.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Loss: 5.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [06:02<00:00, 40.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Loss: 5.3231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:04<00:00, 47.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Loss: 5.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:54<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Loss: 5.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:58<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Loss: 5.3021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14565/14565 [01:20<00:00, 180.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Loss: 5.2968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [04:18<00:00, 56.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Loss: 5.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:43<00:00, 42.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Loss: 5.2869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:49<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Loss: 5.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:47<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Loss: 5.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:55<00:00, 41.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Loss: 5.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:53<00:00, 41.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Loss: 5.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 14565/14565 [05:41<00:00, 42.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Loss: 5.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f0e78d-c0d5-4885-881e-7f528c873c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "def most_similar(word, top_n=5):\n",
    "    if word not in dataset.word2idx:\n",
    "        return []\n",
    "    word_idx = dataset.word2idx[word]\n",
    "    word_vec = word_embeddings[word_idx]\n",
    "    similarities = np.dot(word_embeddings, word_vec) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vec))\n",
    "    most_similar = similarities.argsort()[-top_n-1:-1][::-1]\n",
    "    return [(dataset.idx2word[idx], similarities[idx]) for idx in most_similar if idx != word_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f95f419a-0a5c-4d4f-b892-09c56e3c026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('famous', 0.49498144), ('difference', 0.46292886), ('woman', 0.4597501), ('influential', 0.45435798), ('kingdom', 0.4328415)]\n"
     ]
    }
   ],
   "source": [
    "print(most_similar(\"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2daa26d-37f6-4b9b-95d5-c7f6c3b7229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, 'checkpoints/initial.pkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65a9dbfb-6504-4ce8-affc-c0d8a49045e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/initial.pkg', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a86ffc94-c75a-495b-9215-2e8be9786a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embeddings.weight',\n",
       "              tensor([[-0.0334,  0.7723, -0.1541,  ..., -0.8312, -0.8679, -0.1490],\n",
       "                      [ 0.1979,  0.0738, -0.0707,  ..., -0.6376,  0.0895, -0.0600],\n",
       "                      [-0.1665, -0.0984,  0.0231,  ...,  0.0484,  0.0600, -0.0972],\n",
       "                      ...,\n",
       "                      [-0.0420,  1.2516,  0.3799,  ...,  1.1112,  0.0416,  1.6234],\n",
       "                      [-0.4737,  0.3162,  0.9902,  ...,  0.9877, -0.2667,  0.8550],\n",
       "                      [-0.9057,  0.2380,  0.2295,  ...,  0.0347, -0.9295, -0.0289]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.weight',\n",
       "              tensor([[-0.8265, -0.0731, -0.6963,  ..., -0.0869, -0.9737,  1.0510],\n",
       "                      [-0.7041, -0.2857, -0.5386,  ..., -0.0798, -0.6669,  1.2450],\n",
       "                      [-0.6976, -0.2072, -0.5487,  ..., -0.0528, -0.7097,  1.0454],\n",
       "                      ...,\n",
       "                      [-0.7761, -0.2400, -0.5212,  ...,  0.3490, -0.9173,  1.4619],\n",
       "                      [-0.4572, -0.1212,  0.2855,  ...,  0.2221, -0.6702,  1.4598],\n",
       "                      [-0.8360,  0.1635, -0.6972,  ...,  0.1148, -1.0991,  1.0652]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.9497,  0.3454,  0.8773,  ..., -1.1699, -1.1068, -1.0300],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252cf5f-27b4-406b-ad84-0e869f6a5739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16ef587a-754c-4d06-b46d-fbe42144f187",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74abc57-d2ee-4de4-98e4-0fee8d36c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract month from the 'time' column\n",
    "df['month'] = df['time'].dt.strftime('%B')\n",
    "\n",
    "# Count submissions for each month\n",
    "monthly_submissions = df['month'].value_counts().sort_index()\n",
    "\n",
    "# Define month order for proper sorting\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Reindex the series to ensure all months are included and properly ordered\n",
    "monthly_submissions = monthly_submissions.reindex(month_order).fillna(0)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_submissions.plot(kind='bar')\n",
    "plt.title('Number of Hacker News Submissions per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdace3-1f4a-4dd0-b829-d2f8fa000ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "\n",
    "# Count submissions for each year\n",
    "yearly_submissions = df['year'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_submissions.plot(kind='bar')\n",
    "plt.title('Number of Hacker News Submissions per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.xticks(rotation=0)  # Keeping year labels horizontal\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print the data\n",
    "print(yearly_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd13f7-0125-49a5-8b11-ecee22a33fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by year and month\n",
    "monthly_submissions = monthly_submissions.sort_values(['year', 'month'])\n",
    "\n",
    "# Create a sequential index for x-axis\n",
    "monthly_submissions['index'] = range(len(monthly_submissions))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get unique years for different colors\n",
    "years = monthly_submissions['year'].unique()\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(years))\n",
    "\n",
    "# Plot each year with a different color\n",
    "for year, color in zip(years, colors):\n",
    "    year_data = monthly_submissions[monthly_submissions['year'] == year]\n",
    "    plt.plot(year_data['index'], year_data['count'], label=str(year), color=color)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions per Month (Colored by Year)')\n",
    "plt.xlabel('Months (Sequential)')\n",
    "plt.ylabel('Number of Submissions')\n",
    "\n",
    "# Customize x-axis ticks to show every 12th month (January of each year)\n",
    "xticks = monthly_submissions[monthly_submissions['month'] == 1]['index']\n",
    "xtick_labels = monthly_submissions[monthly_submissions['month'] == 1]['year']\n",
    "plt.xticks(xticks, xtick_labels, rotation=45)\n",
    "\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ade83-c886-4f4c-b919-6bb8e445f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by year and month\n",
    "monthly_submissions = monthly_submissions.sort_values(['year', 'month'])\n",
    "\n",
    "# Create a sequential index for x-axis\n",
    "monthly_submissions['index'] = range(len(monthly_submissions))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Get unique years for different colors\n",
    "years = monthly_submissions['year'].unique()\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(years))\n",
    "\n",
    "# Plot bars for each month, colored by year\n",
    "for year, color in zip(years, colors):\n",
    "    year_data = monthly_submissions[monthly_submissions['year'] == year]\n",
    "    plt.bar(year_data['index'], year_data['count'], color=color, width=1, align='edge')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions per Month (Colored by Year)', fontsize=16)\n",
    "plt.xlabel('Months (Sequential)', fontsize=12)\n",
    "plt.ylabel('Number of Submissions', fontsize=12)\n",
    "\n",
    "# Customize x-axis ticks to show every 12th month (January of each year)\n",
    "xticks = monthly_submissions[monthly_submissions['month'] == 1]['index']\n",
    "xtick_labels = monthly_submissions[monthly_submissions['month'] == 1]['year']\n",
    "plt.xticks(xticks, xtick_labels, rotation=45, ha='right')\n",
    "\n",
    "# Add a legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors]\n",
    "plt.legend(handles, years, title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba779-5362-4540-bea3-4e262b5bb14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02274a43-7f72-4ae8-a376-8784434fecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the dataframe is already loaded as 'df'\n",
    "# If not, you would load it like this:\n",
    "# df = pd.read_csv('hacker_news_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Extract year and month from the 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "# Group by year and month, and count submissions\n",
    "monthly_submissions = df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the data to have years as columns and months as rows\n",
    "pivot_data = monthly_submissions.pivot(index='month', columns='year', values='count')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot lines for each year\n",
    "for year in pivot_data.columns:\n",
    "    plt.plot(pivot_data.index, pivot_data[year], label=str(year), marker='o', markersize=4)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Number of Hacker News Submissions by Month and Year', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Number of Submissions', fontsize=12)\n",
    "\n",
    "# Set x-axis ticks to show month names\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(range(1, 13), month_names)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369f41-7f16-4454-9d1f-797e256c19ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
